{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have made predictions in the way of interpolating by now. Then second thing we want to do it to transform all these category variables in to frequency.(simple one-hot is not very suitable to our data) To be specifically, we'll group our dataset by 'group_1', then calculate frequency of all within single group.(number of 'type' / length of this group).  \n",
    "On the other hand, keep all value type is not a good choice. It will decrease speed and require more memory with low effect on our prediction. When we want a particular type only take up a very limited space in the whole dataset. We will drop the entire column.  \n",
    "At end this step, we should a get a dataframe containing difference between groups. Then, we can mutate other features.(date-related information in different groups, char_38 in different groups, act and people number in different groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from itertools import product\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"leak_train.csv\")\n",
    "test = pd.read_csv(\"leak_test.csv\")\n",
    "data = train.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When apply frequency method, avoid being affected by repeated observation. \n",
    "names = []\n",
    "for name in data.columns:\n",
    "    if name != 'act_activity_id':\n",
    "        names.append(name)\n",
    "#unique_train contrains all valid information\n",
    "unique_data = data[~data.duplicated(subset = names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We transform category variable via group_1 and some group only occured in test set. So we don't know  \n",
    "#the actual return value these groups. We want to extract these group names first.\n",
    "test_groups = test.ppl_group_1[~test.ppl_group_1.isin(train.ppl_group_1.unique())].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#On the forum, they mentioned a important point that we should mutate more features within same group\n",
    "#After we add date-related feature, it's time to do it.\n",
    "mutate_features = {}\n",
    "mutate_features['ppl_group_1'] = []\n",
    "mutate_features['min_group_pdate'] = []\n",
    "mutate_features['max_group_pdate'] = []\n",
    "mutate_features['min_group_adate'] = []\n",
    "mutate_features['max_group_adate'] = []\n",
    "mutate_features['group_adate_range'] = []\n",
    "mutate_features['group_ppl_number'] = []\n",
    "mutate_features['group_act_number'] = []\n",
    "mutate_features['group_adate_number'] = []\n",
    "\n",
    "for g in data.groupby('ppl_group_1'):\n",
    "    mutate_features['ppl_group_1'] .append( g[0] )\n",
    "    mutate_features['min_group_adate'].append( g[1].act_date_diff.min())\n",
    "    mutate_features['max_group_adate'].append( g[1].act_date_diff.max())\n",
    "    mutate_features['min_group_pdate'].append( g[1].ppl_date_diff.min())\n",
    "    mutate_features['max_group_pdate'].append( g[1].ppl_date_diff.max())\n",
    "    mutate_features['group_adate_range'].append( g[1].act_date_diff.max() - g[1].act_date_diff.min())\n",
    "    mutate_features['group_ppl_number'].append(len(g[1]['people_id'].unique()))\n",
    "    mutate_features['group_act_number'].append(len(g[1]['act_activity_id'].unique()))\n",
    "    mutate_features['group_adate_number'].append(len(g[1]['act_date'].unique()))\n",
    "    \n",
    "group_feature = pd.DataFrame(mutate_features)\n",
    "#not necessary to merge\n",
    "#data = pd.merge(data, ppl_in_group, on='ppl_group_1', how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we want to transform these category variables to the form of frequency(general onehot aren't suitable to this)\n",
    "#frequency is very sensitive to repeated observation.\n",
    "\n",
    "\n",
    "\n",
    "#get all category variables' name\n",
    "names = []\n",
    "for i in data.columns:\n",
    "    if 'char' in i and i != 'ppl_char_38' :\n",
    "        names.append(i)\n",
    "    elif i == 'act_activity_category':\n",
    "        names.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we calculate type frequency within each group for each category variables\n",
    "#create a function to do that:\n",
    "def cal_freq(data, key):\n",
    "    freq = {}\n",
    "    #Get allunique possible value in this feature\n",
    "    #col_names =  ['ppl_group_1'] + ('freq_' + data['ppl_char_5'].unique()).tolist()\n",
    "    col_names = ['ppl_group_1'] + data[key].unique().tolist()\n",
    "    a = data[key].unique().tolist()\n",
    "    for n in col_names:\n",
    "        freq[n] = []\n",
    "    for g in data.groupby('ppl_group_1'):\n",
    "        freq['ppl_group_1'].append(g[0])\n",
    "        #get count and corresponding feature name\n",
    "        c = a[:]\n",
    "        value_count = g[1][key].value_counts()\n",
    "        for tp in value_count.index:\n",
    "            freq[tp].append(value_count[tp] / float(len(g[1][key])))\n",
    "            c.remove(tp)\n",
    "        for n in c:\n",
    "            freq[n].append(0)\n",
    "    freq = pd.DataFrame(freq)\n",
    "    a = []\n",
    "    for i in freq.columns:\n",
    "        if i != 'ppl_group_1':\n",
    "            if freq[i].mean > 0.001:\n",
    "                i = key + \"_\"+str(i)\n",
    "                a.append(i)\n",
    "            else:\n",
    "                freq = freq.drop(i)\n",
    "        else:\n",
    "            a.append(i)\n",
    "    freq.columns = a\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in names:\n",
    "    freq = cal_freq(unique_data, key)\n",
    "    group_feature = pd.merge(group_feature, freq, on= 'ppl_group_1', how = 'left')\n",
    "    del freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_feature.to_csv('group_freq.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_feature = pd.read_csv('group_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Then deal with char_38 which is continues variable\n",
    "group_char_38 = {}\n",
    "group_char_38['char_38_mean'] = []\n",
    "group_char_38['char_38_median'] = []\n",
    "group_char_38['char_38_var'] = []\n",
    "group_char_38['ppl_group_1'] = []\n",
    "\n",
    "#assign value\n",
    "for g in unique_data.groupby('ppl_group_1'):\n",
    "    group_char_38['ppl_group_1'].append(g[0])\n",
    "    group_char_38['char_38_mean'].append(g[1]['ppl_char_38'].mean())\n",
    "    group_char_38['char_38_median'].append(g[1]['ppl_char_38'].median())\n",
    "    if len(g[1]) == 1:\n",
    "        group_char_38['char_38_var'].append(0)\n",
    "    else:\n",
    "        group_char_38['char_38_var'].append(g[1]['ppl_char_38'].var())\n",
    "group_char_38 = pd.DataFrame(group_char_38)\n",
    "group_feature = pd.merge(group_feature,group_char_38 , on ='ppl_group_1', how = 'left')\n",
    "del group_char_38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_train =group_feature[~group_feature.ppl_group_1.isin(test_groups)]\n",
    "group_test =group_feature[group_feature.ppl_group_1.isin(test_groups)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We have mutated all features we need. Then we'll divide this group dataframe into train and test. \n",
    "#We can't assign outcome directly for train set, since some group may have different value\n",
    "#But we can transform to all0, all1 and mixed value for all groups\n",
    "group_outcome = {}\n",
    "group_outcome['ppl_group_1'] = []\n",
    "group_outcome['outcome_type'] = []\n",
    "\n",
    "for g in unique_data.groupby('ppl_group_1'):\n",
    "    group_outcome['ppl_group_1'].append(g[0])\n",
    "    m = g[1].outcome.mean()\n",
    "    if m == 1:\n",
    "        group_outcome['outcome_type'].append(1)\n",
    "    elif m == 0:\n",
    "        group_outcome['outcome_type'].append(0)\n",
    "    else:\n",
    "        group_outcome['outcome_type'].append(2)\n",
    "group_outcome = pd.DataFrame(group_outcome)\n",
    "group_train = pd.merge(group_train, group_outcome, on = 'ppl_group_1', how = 'left')\n",
    "del group_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#By now, we have constructed train and test set. \n",
    "#Then create cv set\n",
    "train_y = group_train['outcome_type']\n",
    "train_X = group_train.drop('outcome_type', axis = 1)\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(train_y, 5, test_size=0.2, random_state=42)\n",
    "\n",
    "cv_train_X, cv_train_y = {},{}\n",
    "cv_test_X, cv_test_y = {},{}\n",
    "cv_test_index = {}\n",
    "\n",
    "i = 0\n",
    "for train_index,test_index in sss:\n",
    "    cv_train_X[i], cv_train_y[i] = train_X.iloc[train_index], train_y[train_index]\n",
    "    cv_test_X[i], cv_test_y[i] = train_X.iloc[test_index], train_y[test_index]\n",
    "    cv_test_index[i] = test_index\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813095374667\n",
      "0.80911067243\n",
      "0.816732802912\n",
      "0.818006865455\n",
      "0.811422173413\n"
     ]
    }
   ],
   "source": [
    "#implement classifier on dataset, 5 classifer for 5 split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "rf = {}\n",
    "train_pred1 = []\n",
    "for i in range(len(cv_train_X)):\n",
    "    rf[i] = RandomForestClassifier(warm_start = True, random_state=42) #train same classifier multiple times\n",
    "    for number in range(10,200, 20):\n",
    "        rf[i].set_params(n_estimators = number)\n",
    "        rf[i].fit(cv_train_X[i], cv_train_y[i])\n",
    "    pred = rf[i].predict_proba(cv_test_X[i])\n",
    "    print log_loss(cv_test_y[i], pred)\n",
    "    pred = pd.DataFrame(pred, columns=['all0_pro', 'all1_pro', 'mixed_pro'])\n",
    "    pred['ppl_group_1'] = train_X.iloc[cv_test_index[i]]['ppl_group_1']\n",
    "    train_pred1.append(pred)\n",
    "\n",
    "train_pred1 = pd.concat(train_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810516282386\n",
      "0.81603166933\n",
      "0.816075666662\n",
      "0.81413578847\n",
      "0.816371982218\n"
     ]
    }
   ],
   "source": [
    "rf = {}\n",
    "train_pred2 = []\n",
    "for i in range(len(cv_train_X)):\n",
    "    rf[i] = RandomForestClassifier(warm_start = True, min_samples_split=8,min_samples_leaf=2,n_jobs=8, random_state=42) #train same classifier multiple times\n",
    "    for number in range(10,200, 20):\n",
    "        rf[i].set_params(n_estimators = number)\n",
    "        rf[i].fit(cv_train_X[i], cv_train_y[i])\n",
    "    pred = rf[i].predict_proba(cv_test_X[i])\n",
    "    print log_loss(cv_test_y[i], pred)\n",
    "    pred = pd.DataFrame(pred, columns=['all0_pro', 'all1_pro', 'mixed_pro'])\n",
    "    pred['ppl_group_1'] = train_X.iloc[cv_test_index[i]]['ppl_group_1']\n",
    "    train_pred2.append(pred)\n",
    "\n",
    "train_pred2 = pd.concat(train_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81075512951\n",
      "0.815339952769\n",
      "0.818442570608\n",
      "0.814008939503\n",
      "0.81546569301\n"
     ]
    }
   ],
   "source": [
    "rf = {}\n",
    "train_pred3 = []\n",
    "for i in range(len(cv_train_X)):\n",
    "    rf[i] = RandomForestClassifier(warm_start = True, min_samples_split=12,min_samples_leaf=2,n_jobs=8, random_state=42) #train same classifier multiple times\n",
    "    for number in range(10,200, 20):\n",
    "        rf[i].set_params(n_estimators = number)\n",
    "        rf[i].fit(cv_train_X[i], cv_train_y[i])\n",
    "    pred = rf[i].predict_proba(cv_test_X[i])\n",
    "    print log_loss(cv_test_y[i], pred)\n",
    "    pred = pd.DataFrame(pred, columns=['all0_pro', 'all1_pro', 'mixed_pro'])\n",
    "    pred['ppl_group_1'] = train_X.iloc[cv_test_index[i]]['ppl_group_1']\n",
    "    train_pred3.append(pred)\n",
    "\n",
    "train_pred3 = pd.concat(train_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xg_train = {}\n",
    "xg_test = {}\n",
    "for i in range(5):\n",
    "    xg_train[i] = xgb.DMatrix( cv_train_X[i], label= cv_train_y[i])\n",
    "    xg_test[i] = xgb.DMatrix(cv_test_X[i], label= cv_test_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09501\ttest-mlogloss:1.09514\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[10]\ttrain-mlogloss:1.0608\ttest-mlogloss:1.06195\n",
      "[20]\ttrain-mlogloss:1.03055\ttest-mlogloss:1.03274\n",
      "[30]\ttrain-mlogloss:1.00409\ttest-mlogloss:1.00731\n",
      "[40]\ttrain-mlogloss:0.98031\ttest-mlogloss:0.984481\n",
      "[50]\ttrain-mlogloss:0.958882\ttest-mlogloss:0.964266\n",
      "[60]\ttrain-mlogloss:0.93996\ttest-mlogloss:0.946385\n",
      "[70]\ttrain-mlogloss:0.921829\ttest-mlogloss:0.929494\n",
      "[80]\ttrain-mlogloss:0.905611\ttest-mlogloss:0.91443\n",
      "[90]\ttrain-mlogloss:0.890927\ttest-mlogloss:0.900988\n",
      "[100]\ttrain-mlogloss:0.8776\ttest-mlogloss:0.888691\n",
      "[110]\ttrain-mlogloss:0.865203\ttest-mlogloss:0.877586\n",
      "[120]\ttrain-mlogloss:0.854466\ttest-mlogloss:0.868052\n",
      "[130]\ttrain-mlogloss:0.844143\ttest-mlogloss:0.858922\n",
      "[140]\ttrain-mlogloss:0.834563\ttest-mlogloss:0.850639\n",
      "[150]\ttrain-mlogloss:0.825695\ttest-mlogloss:0.842848\n",
      "[160]\ttrain-mlogloss:0.817246\ttest-mlogloss:0.8357\n",
      "[170]\ttrain-mlogloss:0.809255\ttest-mlogloss:0.829047\n",
      "[180]\ttrain-mlogloss:0.801954\ttest-mlogloss:0.823072\n",
      "[190]\ttrain-mlogloss:0.795128\ttest-mlogloss:0.817435\n",
      "[200]\ttrain-mlogloss:0.788795\ttest-mlogloss:0.812257\n",
      "[210]\ttrain-mlogloss:0.782895\ttest-mlogloss:0.807629\n",
      "[220]\ttrain-mlogloss:0.777214\ttest-mlogloss:0.803207\n",
      "[230]\ttrain-mlogloss:0.771776\ttest-mlogloss:0.799142\n",
      "[240]\ttrain-mlogloss:0.76679\ttest-mlogloss:0.795324\n",
      "[250]\ttrain-mlogloss:0.761851\ttest-mlogloss:0.791736\n",
      "[260]\ttrain-mlogloss:0.757276\ttest-mlogloss:0.788406\n",
      "[270]\ttrain-mlogloss:0.752814\ttest-mlogloss:0.785241\n",
      "[280]\ttrain-mlogloss:0.748476\ttest-mlogloss:0.782203\n",
      "[290]\ttrain-mlogloss:0.744584\ttest-mlogloss:0.779414\n",
      "[300]\ttrain-mlogloss:0.740742\ttest-mlogloss:0.776805\n",
      "[310]\ttrain-mlogloss:0.736791\ttest-mlogloss:0.774275\n",
      "[320]\ttrain-mlogloss:0.733122\ttest-mlogloss:0.771932\n",
      "[330]\ttrain-mlogloss:0.729849\ttest-mlogloss:0.769895\n",
      "[340]\ttrain-mlogloss:0.726441\ttest-mlogloss:0.767807\n",
      "[350]\ttrain-mlogloss:0.723162\ttest-mlogloss:0.765827\n",
      "[360]\ttrain-mlogloss:0.720157\ttest-mlogloss:0.764076\n",
      "[370]\ttrain-mlogloss:0.717204\ttest-mlogloss:0.762434\n",
      "[380]\ttrain-mlogloss:0.714257\ttest-mlogloss:0.760806\n",
      "[390]\ttrain-mlogloss:0.711438\ttest-mlogloss:0.759204\n",
      "[400]\ttrain-mlogloss:0.708838\ttest-mlogloss:0.757843\n",
      "[410]\ttrain-mlogloss:0.706217\ttest-mlogloss:0.756367\n",
      "[420]\ttrain-mlogloss:0.703793\ttest-mlogloss:0.755091\n",
      "[430]\ttrain-mlogloss:0.701489\ttest-mlogloss:0.753937\n",
      "[440]\ttrain-mlogloss:0.699211\ttest-mlogloss:0.752919\n",
      "[450]\ttrain-mlogloss:0.696806\ttest-mlogloss:0.751659\n",
      "[460]\ttrain-mlogloss:0.694469\ttest-mlogloss:0.750472\n",
      "[470]\ttrain-mlogloss:0.692385\ttest-mlogloss:0.749461\n",
      "[480]\ttrain-mlogloss:0.69035\ttest-mlogloss:0.748558\n",
      "[490]\ttrain-mlogloss:0.68848\ttest-mlogloss:0.747678\n",
      "[500]\ttrain-mlogloss:0.686464\ttest-mlogloss:0.746633\n",
      "[510]\ttrain-mlogloss:0.684677\ttest-mlogloss:0.745811\n",
      "[520]\ttrain-mlogloss:0.682897\ttest-mlogloss:0.745045\n",
      "[530]\ttrain-mlogloss:0.681012\ttest-mlogloss:0.744275\n",
      "[540]\ttrain-mlogloss:0.679094\ttest-mlogloss:0.743458\n",
      "[550]\ttrain-mlogloss:0.677425\ttest-mlogloss:0.742769\n",
      "[560]\ttrain-mlogloss:0.675551\ttest-mlogloss:0.741955\n",
      "[570]\ttrain-mlogloss:0.673947\ttest-mlogloss:0.741267\n",
      "[580]\ttrain-mlogloss:0.672302\ttest-mlogloss:0.740572\n",
      "[590]\ttrain-mlogloss:0.670805\ttest-mlogloss:0.740001\n",
      "[600]\ttrain-mlogloss:0.66914\ttest-mlogloss:0.739349\n",
      "[610]\ttrain-mlogloss:0.667629\ttest-mlogloss:0.738769\n",
      "[620]\ttrain-mlogloss:0.666291\ttest-mlogloss:0.738208\n",
      "[630]\ttrain-mlogloss:0.6649\ttest-mlogloss:0.73763\n",
      "[640]\ttrain-mlogloss:0.663503\ttest-mlogloss:0.737168\n",
      "[650]\ttrain-mlogloss:0.662336\ttest-mlogloss:0.736797\n",
      "[660]\ttrain-mlogloss:0.661094\ttest-mlogloss:0.736358\n",
      "[670]\ttrain-mlogloss:0.659649\ttest-mlogloss:0.735819\n",
      "[680]\ttrain-mlogloss:0.658031\ttest-mlogloss:0.735133\n",
      "[690]\ttrain-mlogloss:0.656849\ttest-mlogloss:0.734804\n",
      "[700]\ttrain-mlogloss:0.655614\ttest-mlogloss:0.734459\n",
      "[710]\ttrain-mlogloss:0.654283\ttest-mlogloss:0.73403\n",
      "[720]\ttrain-mlogloss:0.653125\ttest-mlogloss:0.733679\n",
      "[730]\ttrain-mlogloss:0.651892\ttest-mlogloss:0.733292\n",
      "[740]\ttrain-mlogloss:0.650674\ttest-mlogloss:0.73295\n",
      "[750]\ttrain-mlogloss:0.649519\ttest-mlogloss:0.732634\n",
      "[760]\ttrain-mlogloss:0.648322\ttest-mlogloss:0.732268\n",
      "[770]\ttrain-mlogloss:0.647159\ttest-mlogloss:0.73199\n",
      "[780]\ttrain-mlogloss:0.646041\ttest-mlogloss:0.731616\n",
      "[790]\ttrain-mlogloss:0.644872\ttest-mlogloss:0.731268\n",
      "[800]\ttrain-mlogloss:0.643851\ttest-mlogloss:0.731007\n",
      "[810]\ttrain-mlogloss:0.642831\ttest-mlogloss:0.730798\n",
      "[820]\ttrain-mlogloss:0.641814\ttest-mlogloss:0.730576\n",
      "[830]\ttrain-mlogloss:0.640594\ttest-mlogloss:0.730242\n",
      "[840]\ttrain-mlogloss:0.639631\ttest-mlogloss:0.730006\n",
      "[850]\ttrain-mlogloss:0.638558\ttest-mlogloss:0.72966\n",
      "[860]\ttrain-mlogloss:0.637502\ttest-mlogloss:0.729391\n",
      "[870]\ttrain-mlogloss:0.636535\ttest-mlogloss:0.729182\n",
      "[880]\ttrain-mlogloss:0.635621\ttest-mlogloss:0.729002\n",
      "[890]\ttrain-mlogloss:0.6346\ttest-mlogloss:0.728769\n",
      "[900]\ttrain-mlogloss:0.633636\ttest-mlogloss:0.728577\n",
      "[910]\ttrain-mlogloss:0.632684\ttest-mlogloss:0.728418\n",
      "[920]\ttrain-mlogloss:0.631748\ttest-mlogloss:0.728216\n",
      "[930]\ttrain-mlogloss:0.630808\ttest-mlogloss:0.728061\n",
      "[940]\ttrain-mlogloss:0.629947\ttest-mlogloss:0.727907\n",
      "[950]\ttrain-mlogloss:0.629059\ttest-mlogloss:0.727726\n",
      "[960]\ttrain-mlogloss:0.628087\ttest-mlogloss:0.727527\n",
      "[970]\ttrain-mlogloss:0.62709\ttest-mlogloss:0.727409\n",
      "[980]\ttrain-mlogloss:0.626139\ttest-mlogloss:0.727275\n",
      "[990]\ttrain-mlogloss:0.625087\ttest-mlogloss:0.727037\n",
      "[1000]\ttrain-mlogloss:0.624178\ttest-mlogloss:0.726819\n",
      "[1010]\ttrain-mlogloss:0.623207\ttest-mlogloss:0.726628\n",
      "[1020]\ttrain-mlogloss:0.622315\ttest-mlogloss:0.726421\n",
      "[1030]\ttrain-mlogloss:0.621311\ttest-mlogloss:0.72629\n",
      "[1040]\ttrain-mlogloss:0.620407\ttest-mlogloss:0.726214\n",
      "[1050]\ttrain-mlogloss:0.619478\ttest-mlogloss:0.726011\n",
      "[1060]\ttrain-mlogloss:0.618633\ttest-mlogloss:0.725841\n",
      "[1070]\ttrain-mlogloss:0.617841\ttest-mlogloss:0.72573\n",
      "[1080]\ttrain-mlogloss:0.617038\ttest-mlogloss:0.725657\n",
      "[1090]\ttrain-mlogloss:0.615938\ttest-mlogloss:0.725514\n",
      "[1100]\ttrain-mlogloss:0.615149\ttest-mlogloss:0.725421\n",
      "[1110]\ttrain-mlogloss:0.614225\ttest-mlogloss:0.725269\n",
      "[1120]\ttrain-mlogloss:0.613446\ttest-mlogloss:0.725155\n",
      "[1130]\ttrain-mlogloss:0.612614\ttest-mlogloss:0.725009\n",
      "[1140]\ttrain-mlogloss:0.611711\ttest-mlogloss:0.724896\n",
      "[1150]\ttrain-mlogloss:0.611005\ttest-mlogloss:0.724828\n",
      "[1160]\ttrain-mlogloss:0.610246\ttest-mlogloss:0.724734\n",
      "[1170]\ttrain-mlogloss:0.609562\ttest-mlogloss:0.724728\n",
      "[1180]\ttrain-mlogloss:0.608832\ttest-mlogloss:0.724634\n",
      "[1190]\ttrain-mlogloss:0.608077\ttest-mlogloss:0.724545\n",
      "[1200]\ttrain-mlogloss:0.607414\ttest-mlogloss:0.724483\n",
      "[1210]\ttrain-mlogloss:0.606613\ttest-mlogloss:0.724376\n",
      "[1220]\ttrain-mlogloss:0.605811\ttest-mlogloss:0.724243\n",
      "[1230]\ttrain-mlogloss:0.605017\ttest-mlogloss:0.724161\n",
      "[1240]\ttrain-mlogloss:0.60416\ttest-mlogloss:0.724037\n",
      "[1250]\ttrain-mlogloss:0.603384\ttest-mlogloss:0.723941\n",
      "[1260]\ttrain-mlogloss:0.602571\ttest-mlogloss:0.723862\n",
      "[1270]\ttrain-mlogloss:0.601857\ttest-mlogloss:0.723781\n",
      "[1280]\ttrain-mlogloss:0.601173\ttest-mlogloss:0.72371\n",
      "[1290]\ttrain-mlogloss:0.600527\ttest-mlogloss:0.723676\n",
      "[1300]\ttrain-mlogloss:0.599857\ttest-mlogloss:0.72368\n",
      "[1310]\ttrain-mlogloss:0.599184\ttest-mlogloss:0.723592\n",
      "[1320]\ttrain-mlogloss:0.598396\ttest-mlogloss:0.723485\n",
      "[1330]\ttrain-mlogloss:0.597664\ttest-mlogloss:0.723407\n",
      "[1340]\ttrain-mlogloss:0.596942\ttest-mlogloss:0.723307\n",
      "[1350]\ttrain-mlogloss:0.596291\ttest-mlogloss:0.723236\n",
      "[1360]\ttrain-mlogloss:0.595334\ttest-mlogloss:0.723116\n",
      "[1370]\ttrain-mlogloss:0.59456\ttest-mlogloss:0.723009\n",
      "[1380]\ttrain-mlogloss:0.593817\ttest-mlogloss:0.722889\n",
      "[1390]\ttrain-mlogloss:0.593162\ttest-mlogloss:0.722864\n",
      "[1400]\ttrain-mlogloss:0.592377\ttest-mlogloss:0.722771\n",
      "[1410]\ttrain-mlogloss:0.591737\ttest-mlogloss:0.722738\n",
      "[1420]\ttrain-mlogloss:0.591106\ttest-mlogloss:0.722628\n",
      "[1430]\ttrain-mlogloss:0.590383\ttest-mlogloss:0.722537\n",
      "[1440]\ttrain-mlogloss:0.58976\ttest-mlogloss:0.722482\n",
      "[1450]\ttrain-mlogloss:0.589048\ttest-mlogloss:0.72242\n",
      "[1460]\ttrain-mlogloss:0.588448\ttest-mlogloss:0.722371\n",
      "[1470]\ttrain-mlogloss:0.587734\ttest-mlogloss:0.722316\n",
      "[1480]\ttrain-mlogloss:0.587048\ttest-mlogloss:0.722252\n",
      "[1490]\ttrain-mlogloss:0.586341\ttest-mlogloss:0.722238\n",
      "[1500]\ttrain-mlogloss:0.585669\ttest-mlogloss:0.72215\n",
      "[1510]\ttrain-mlogloss:0.585047\ttest-mlogloss:0.722099\n",
      "[1520]\ttrain-mlogloss:0.584343\ttest-mlogloss:0.722072\n",
      "[1530]\ttrain-mlogloss:0.583641\ttest-mlogloss:0.72205\n",
      "[1540]\ttrain-mlogloss:0.582978\ttest-mlogloss:0.722042\n",
      "[1550]\ttrain-mlogloss:0.582393\ttest-mlogloss:0.721986\n",
      "[1560]\ttrain-mlogloss:0.581761\ttest-mlogloss:0.721995\n",
      "[1570]\ttrain-mlogloss:0.581045\ttest-mlogloss:0.721977\n",
      "[1580]\ttrain-mlogloss:0.580462\ttest-mlogloss:0.7219\n",
      "[1590]\ttrain-mlogloss:0.579813\ttest-mlogloss:0.721872\n",
      "[1600]\ttrain-mlogloss:0.579202\ttest-mlogloss:0.721882\n",
      "[1610]\ttrain-mlogloss:0.578585\ttest-mlogloss:0.721827\n",
      "[1620]\ttrain-mlogloss:0.577975\ttest-mlogloss:0.721812\n",
      "[1630]\ttrain-mlogloss:0.577345\ttest-mlogloss:0.721794\n",
      "[1640]\ttrain-mlogloss:0.57679\ttest-mlogloss:0.72181\n",
      "[1650]\ttrain-mlogloss:0.576133\ttest-mlogloss:0.7218\n",
      "[1660]\ttrain-mlogloss:0.575459\ttest-mlogloss:0.721793\n",
      "[1670]\ttrain-mlogloss:0.57491\ttest-mlogloss:0.721793\n",
      "[1680]\ttrain-mlogloss:0.574321\ttest-mlogloss:0.721791\n",
      "[1690]\ttrain-mlogloss:0.573712\ttest-mlogloss:0.721786\n",
      "[1700]\ttrain-mlogloss:0.573073\ttest-mlogloss:0.721709\n",
      "[1710]\ttrain-mlogloss:0.57245\ttest-mlogloss:0.721637\n",
      "[1720]\ttrain-mlogloss:0.571731\ttest-mlogloss:0.721587\n",
      "[1730]\ttrain-mlogloss:0.571132\ttest-mlogloss:0.721535\n",
      "[1740]\ttrain-mlogloss:0.570536\ttest-mlogloss:0.72148\n",
      "[1750]\ttrain-mlogloss:0.569969\ttest-mlogloss:0.72148\n",
      "[1760]\ttrain-mlogloss:0.569357\ttest-mlogloss:0.72144\n",
      "[1770]\ttrain-mlogloss:0.568646\ttest-mlogloss:0.721385\n",
      "[1780]\ttrain-mlogloss:0.568059\ttest-mlogloss:0.721362\n",
      "[1790]\ttrain-mlogloss:0.567485\ttest-mlogloss:0.721307\n",
      "[1800]\ttrain-mlogloss:0.566855\ttest-mlogloss:0.72128\n",
      "[1810]\ttrain-mlogloss:0.566203\ttest-mlogloss:0.721221\n",
      "[1820]\ttrain-mlogloss:0.565618\ttest-mlogloss:0.721185\n",
      "[1830]\ttrain-mlogloss:0.564981\ttest-mlogloss:0.72117\n",
      "[1840]\ttrain-mlogloss:0.564407\ttest-mlogloss:0.721183\n",
      "[1850]\ttrain-mlogloss:0.563909\ttest-mlogloss:0.721156\n",
      "[1860]\ttrain-mlogloss:0.563253\ttest-mlogloss:0.721087\n",
      "[1870]\ttrain-mlogloss:0.562699\ttest-mlogloss:0.72105\n",
      "[1880]\ttrain-mlogloss:0.562064\ttest-mlogloss:0.721024\n",
      "[1890]\ttrain-mlogloss:0.561489\ttest-mlogloss:0.720989\n",
      "[1900]\ttrain-mlogloss:0.560927\ttest-mlogloss:0.720978\n",
      "[1910]\ttrain-mlogloss:0.560348\ttest-mlogloss:0.720971\n",
      "[1920]\ttrain-mlogloss:0.559739\ttest-mlogloss:0.720967\n",
      "[1930]\ttrain-mlogloss:0.559119\ttest-mlogloss:0.72094\n",
      "[1940]\ttrain-mlogloss:0.558614\ttest-mlogloss:0.720953\n",
      "[1950]\ttrain-mlogloss:0.558008\ttest-mlogloss:0.720907\n",
      "[1960]\ttrain-mlogloss:0.557478\ttest-mlogloss:0.720879\n",
      "[1970]\ttrain-mlogloss:0.556873\ttest-mlogloss:0.72084\n",
      "[1980]\ttrain-mlogloss:0.556282\ttest-mlogloss:0.720794\n",
      "[1990]\ttrain-mlogloss:0.555731\ttest-mlogloss:0.720789\n",
      "[2000]\ttrain-mlogloss:0.555172\ttest-mlogloss:0.720772\n",
      "[2010]\ttrain-mlogloss:0.55455\ttest-mlogloss:0.720801\n",
      "[2020]\ttrain-mlogloss:0.553932\ttest-mlogloss:0.720801\n",
      "[2030]\ttrain-mlogloss:0.553355\ttest-mlogloss:0.720798\n",
      "Stopping. Best iteration:\n",
      "[2003]\ttrain-mlogloss:0.554957\ttest-mlogloss:0.720769\n",
      "\n",
      "[0]\ttrain-mlogloss:1.09494\ttest-mlogloss:1.09508\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[10]\ttrain-mlogloss:1.06058\ttest-mlogloss:1.06217\n",
      "[20]\ttrain-mlogloss:1.03068\ttest-mlogloss:1.0335\n",
      "[30]\ttrain-mlogloss:1.00392\ttest-mlogloss:1.00808\n",
      "[40]\ttrain-mlogloss:0.979847\ttest-mlogloss:0.98544\n",
      "[50]\ttrain-mlogloss:0.958187\ttest-mlogloss:0.965203\n",
      "[60]\ttrain-mlogloss:0.93925\ttest-mlogloss:0.947633\n",
      "[70]\ttrain-mlogloss:0.921059\ttest-mlogloss:0.930936\n",
      "[80]\ttrain-mlogloss:0.904955\ttest-mlogloss:0.916339\n",
      "[90]\ttrain-mlogloss:0.890378\ttest-mlogloss:0.903028\n",
      "[100]\ttrain-mlogloss:0.877166\ttest-mlogloss:0.891079\n",
      "[110]\ttrain-mlogloss:0.864866\ttest-mlogloss:0.880171\n",
      "[120]\ttrain-mlogloss:0.854052\ttest-mlogloss:0.870819\n",
      "[130]\ttrain-mlogloss:0.843513\ttest-mlogloss:0.861727\n",
      "[140]\ttrain-mlogloss:0.833947\ttest-mlogloss:0.853451\n",
      "[150]\ttrain-mlogloss:0.824917\ttest-mlogloss:0.845863\n",
      "[160]\ttrain-mlogloss:0.816433\ttest-mlogloss:0.838861\n",
      "[170]\ttrain-mlogloss:0.808551\ttest-mlogloss:0.832408\n",
      "[180]\ttrain-mlogloss:0.801232\ttest-mlogloss:0.826592\n",
      "[190]\ttrain-mlogloss:0.794465\ttest-mlogloss:0.821296\n",
      "[200]\ttrain-mlogloss:0.788155\ttest-mlogloss:0.816303\n",
      "[210]\ttrain-mlogloss:0.782185\ttest-mlogloss:0.811874\n",
      "[220]\ttrain-mlogloss:0.776375\ttest-mlogloss:0.807605\n",
      "[230]\ttrain-mlogloss:0.770852\ttest-mlogloss:0.803619\n",
      "[240]\ttrain-mlogloss:0.765733\ttest-mlogloss:0.800007\n",
      "[250]\ttrain-mlogloss:0.76068\ttest-mlogloss:0.796368\n",
      "[260]\ttrain-mlogloss:0.756152\ttest-mlogloss:0.793159\n",
      "[270]\ttrain-mlogloss:0.751675\ttest-mlogloss:0.790081\n",
      "[280]\ttrain-mlogloss:0.747305\ttest-mlogloss:0.78705\n",
      "[290]\ttrain-mlogloss:0.743409\ttest-mlogloss:0.78456\n",
      "[300]\ttrain-mlogloss:0.73952\ttest-mlogloss:0.782111\n",
      "[310]\ttrain-mlogloss:0.735703\ttest-mlogloss:0.779647\n",
      "[320]\ttrain-mlogloss:0.732116\ttest-mlogloss:0.77734\n",
      "[330]\ttrain-mlogloss:0.728629\ttest-mlogloss:0.775252\n",
      "[340]\ttrain-mlogloss:0.725289\ttest-mlogloss:0.773229\n",
      "[350]\ttrain-mlogloss:0.721864\ttest-mlogloss:0.771044\n",
      "[360]\ttrain-mlogloss:0.718721\ttest-mlogloss:0.769254\n",
      "[370]\ttrain-mlogloss:0.715831\ttest-mlogloss:0.767651\n",
      "[380]\ttrain-mlogloss:0.712946\ttest-mlogloss:0.766052\n",
      "[390]\ttrain-mlogloss:0.710049\ttest-mlogloss:0.764476\n",
      "[400]\ttrain-mlogloss:0.707456\ttest-mlogloss:0.763132\n",
      "[410]\ttrain-mlogloss:0.704916\ttest-mlogloss:0.761794\n",
      "[420]\ttrain-mlogloss:0.702389\ttest-mlogloss:0.760509\n",
      "[430]\ttrain-mlogloss:0.700158\ttest-mlogloss:0.759417\n",
      "[440]\ttrain-mlogloss:0.697762\ttest-mlogloss:0.758225\n",
      "[450]\ttrain-mlogloss:0.695517\ttest-mlogloss:0.757177\n",
      "[460]\ttrain-mlogloss:0.693435\ttest-mlogloss:0.756217\n",
      "[470]\ttrain-mlogloss:0.691479\ttest-mlogloss:0.755361\n",
      "[480]\ttrain-mlogloss:0.689479\ttest-mlogloss:0.75428\n",
      "[490]\ttrain-mlogloss:0.687465\ttest-mlogloss:0.753487\n",
      "[500]\ttrain-mlogloss:0.685727\ttest-mlogloss:0.752732\n",
      "[510]\ttrain-mlogloss:0.683771\ttest-mlogloss:0.751854\n",
      "[520]\ttrain-mlogloss:0.681832\ttest-mlogloss:0.751097\n",
      "[530]\ttrain-mlogloss:0.680123\ttest-mlogloss:0.750404\n",
      "[540]\ttrain-mlogloss:0.678369\ttest-mlogloss:0.749761\n",
      "[550]\ttrain-mlogloss:0.676731\ttest-mlogloss:0.749081\n",
      "[560]\ttrain-mlogloss:0.674847\ttest-mlogloss:0.748258\n",
      "[570]\ttrain-mlogloss:0.673274\ttest-mlogloss:0.747656\n",
      "[580]\ttrain-mlogloss:0.671717\ttest-mlogloss:0.747067\n",
      "[590]\ttrain-mlogloss:0.670116\ttest-mlogloss:0.746444\n",
      "[600]\ttrain-mlogloss:0.66865\ttest-mlogloss:0.745931\n",
      "[610]\ttrain-mlogloss:0.667126\ttest-mlogloss:0.745423\n",
      "[620]\ttrain-mlogloss:0.665429\ttest-mlogloss:0.744812\n",
      "[630]\ttrain-mlogloss:0.663999\ttest-mlogloss:0.744346\n",
      "[640]\ttrain-mlogloss:0.662595\ttest-mlogloss:0.743886\n",
      "[650]\ttrain-mlogloss:0.661214\ttest-mlogloss:0.743445\n",
      "[660]\ttrain-mlogloss:0.659826\ttest-mlogloss:0.742932\n",
      "[670]\ttrain-mlogloss:0.658569\ttest-mlogloss:0.742501\n",
      "[680]\ttrain-mlogloss:0.657166\ttest-mlogloss:0.742048\n",
      "[690]\ttrain-mlogloss:0.656055\ttest-mlogloss:0.741735\n",
      "[700]\ttrain-mlogloss:0.654886\ttest-mlogloss:0.741491\n",
      "[710]\ttrain-mlogloss:0.653589\ttest-mlogloss:0.741159\n",
      "[720]\ttrain-mlogloss:0.652142\ttest-mlogloss:0.740682\n",
      "[730]\ttrain-mlogloss:0.651034\ttest-mlogloss:0.740428\n",
      "[740]\ttrain-mlogloss:0.649925\ttest-mlogloss:0.740183\n",
      "[750]\ttrain-mlogloss:0.648695\ttest-mlogloss:0.739799\n",
      "[760]\ttrain-mlogloss:0.647557\ttest-mlogloss:0.739506\n",
      "[770]\ttrain-mlogloss:0.646205\ttest-mlogloss:0.739191\n",
      "[780]\ttrain-mlogloss:0.644939\ttest-mlogloss:0.738785\n",
      "[790]\ttrain-mlogloss:0.643869\ttest-mlogloss:0.738503\n",
      "[800]\ttrain-mlogloss:0.642683\ttest-mlogloss:0.738264\n",
      "[810]\ttrain-mlogloss:0.641804\ttest-mlogloss:0.738041\n",
      "[820]\ttrain-mlogloss:0.640595\ttest-mlogloss:0.73768\n",
      "[830]\ttrain-mlogloss:0.639489\ttest-mlogloss:0.737331\n",
      "[840]\ttrain-mlogloss:0.638467\ttest-mlogloss:0.737166\n",
      "[850]\ttrain-mlogloss:0.637351\ttest-mlogloss:0.736851\n",
      "[860]\ttrain-mlogloss:0.636384\ttest-mlogloss:0.736716\n",
      "[870]\ttrain-mlogloss:0.635375\ttest-mlogloss:0.736495\n",
      "[880]\ttrain-mlogloss:0.634358\ttest-mlogloss:0.736269\n",
      "[890]\ttrain-mlogloss:0.633427\ttest-mlogloss:0.736115\n",
      "[900]\ttrain-mlogloss:0.632459\ttest-mlogloss:0.735907\n",
      "[910]\ttrain-mlogloss:0.631403\ttest-mlogloss:0.735685\n",
      "[920]\ttrain-mlogloss:0.630468\ttest-mlogloss:0.735499\n",
      "[930]\ttrain-mlogloss:0.629588\ttest-mlogloss:0.735355\n",
      "[940]\ttrain-mlogloss:0.628722\ttest-mlogloss:0.735257\n",
      "[950]\ttrain-mlogloss:0.627824\ttest-mlogloss:0.735192\n",
      "[960]\ttrain-mlogloss:0.626911\ttest-mlogloss:0.735075\n",
      "[970]\ttrain-mlogloss:0.626024\ttest-mlogloss:0.734961\n",
      "[980]\ttrain-mlogloss:0.625316\ttest-mlogloss:0.734869\n",
      "[990]\ttrain-mlogloss:0.624309\ttest-mlogloss:0.734599\n",
      "[1000]\ttrain-mlogloss:0.623398\ttest-mlogloss:0.734464\n",
      "[1010]\ttrain-mlogloss:0.622463\ttest-mlogloss:0.734275\n",
      "[1020]\ttrain-mlogloss:0.62168\ttest-mlogloss:0.734182\n",
      "[1030]\ttrain-mlogloss:0.620744\ttest-mlogloss:0.734031\n",
      "[1040]\ttrain-mlogloss:0.619827\ttest-mlogloss:0.733916\n",
      "[1050]\ttrain-mlogloss:0.618928\ttest-mlogloss:0.733783\n",
      "[1060]\ttrain-mlogloss:0.618172\ttest-mlogloss:0.733673\n",
      "[1070]\ttrain-mlogloss:0.617335\ttest-mlogloss:0.733575\n",
      "[1080]\ttrain-mlogloss:0.616464\ttest-mlogloss:0.733464\n",
      "[1090]\ttrain-mlogloss:0.615491\ttest-mlogloss:0.733259\n",
      "[1100]\ttrain-mlogloss:0.614694\ttest-mlogloss:0.733176\n",
      "[1110]\ttrain-mlogloss:0.61391\ttest-mlogloss:0.73307\n",
      "[1120]\ttrain-mlogloss:0.613197\ttest-mlogloss:0.733003\n",
      "[1130]\ttrain-mlogloss:0.612478\ttest-mlogloss:0.73296\n",
      "[1140]\ttrain-mlogloss:0.611578\ttest-mlogloss:0.732819\n",
      "[1150]\ttrain-mlogloss:0.610705\ttest-mlogloss:0.732661\n",
      "[1160]\ttrain-mlogloss:0.609918\ttest-mlogloss:0.732571\n",
      "[1170]\ttrain-mlogloss:0.609136\ttest-mlogloss:0.732494\n",
      "[1180]\ttrain-mlogloss:0.608369\ttest-mlogloss:0.732399\n",
      "[1190]\ttrain-mlogloss:0.607538\ttest-mlogloss:0.732237\n",
      "[1200]\ttrain-mlogloss:0.606778\ttest-mlogloss:0.732115\n",
      "[1210]\ttrain-mlogloss:0.606094\ttest-mlogloss:0.732033\n",
      "[1220]\ttrain-mlogloss:0.605336\ttest-mlogloss:0.731974\n",
      "[1230]\ttrain-mlogloss:0.604581\ttest-mlogloss:0.731893\n",
      "[1240]\ttrain-mlogloss:0.603899\ttest-mlogloss:0.731807\n",
      "[1250]\ttrain-mlogloss:0.603237\ttest-mlogloss:0.731776\n",
      "[1260]\ttrain-mlogloss:0.60239\ttest-mlogloss:0.731667\n",
      "[1270]\ttrain-mlogloss:0.601747\ttest-mlogloss:0.7316\n",
      "[1280]\ttrain-mlogloss:0.601001\ttest-mlogloss:0.731513\n",
      "[1290]\ttrain-mlogloss:0.600336\ttest-mlogloss:0.731452\n",
      "[1300]\ttrain-mlogloss:0.59957\ttest-mlogloss:0.73143\n",
      "[1310]\ttrain-mlogloss:0.598838\ttest-mlogloss:0.731385\n",
      "[1320]\ttrain-mlogloss:0.598131\ttest-mlogloss:0.731375\n",
      "[1330]\ttrain-mlogloss:0.597484\ttest-mlogloss:0.731359\n",
      "[1340]\ttrain-mlogloss:0.596581\ttest-mlogloss:0.731277\n",
      "[1350]\ttrain-mlogloss:0.595829\ttest-mlogloss:0.731203\n",
      "[1360]\ttrain-mlogloss:0.595105\ttest-mlogloss:0.731115\n",
      "[1370]\ttrain-mlogloss:0.594378\ttest-mlogloss:0.731063\n",
      "[1380]\ttrain-mlogloss:0.593627\ttest-mlogloss:0.730989\n",
      "[1390]\ttrain-mlogloss:0.592996\ttest-mlogloss:0.73098\n",
      "[1400]\ttrain-mlogloss:0.592323\ttest-mlogloss:0.73093\n",
      "[1410]\ttrain-mlogloss:0.591611\ttest-mlogloss:0.730926\n",
      "[1420]\ttrain-mlogloss:0.590952\ttest-mlogloss:0.730902\n",
      "[1430]\ttrain-mlogloss:0.590269\ttest-mlogloss:0.730857\n",
      "[1440]\ttrain-mlogloss:0.589541\ttest-mlogloss:0.73079\n",
      "[1450]\ttrain-mlogloss:0.588938\ttest-mlogloss:0.730795\n",
      "[1460]\ttrain-mlogloss:0.588241\ttest-mlogloss:0.730784\n",
      "[1470]\ttrain-mlogloss:0.58758\ttest-mlogloss:0.730741\n",
      "[1480]\ttrain-mlogloss:0.586974\ttest-mlogloss:0.730692\n",
      "[1490]\ttrain-mlogloss:0.586212\ttest-mlogloss:0.73067\n",
      "[1500]\ttrain-mlogloss:0.585489\ttest-mlogloss:0.730639\n",
      "[1510]\ttrain-mlogloss:0.584799\ttest-mlogloss:0.730586\n",
      "[1520]\ttrain-mlogloss:0.584076\ttest-mlogloss:0.73055\n",
      "[1530]\ttrain-mlogloss:0.583442\ttest-mlogloss:0.730497\n",
      "[1540]\ttrain-mlogloss:0.58275\ttest-mlogloss:0.730465\n",
      "[1550]\ttrain-mlogloss:0.582085\ttest-mlogloss:0.730404\n",
      "[1560]\ttrain-mlogloss:0.581486\ttest-mlogloss:0.73033\n",
      "[1570]\ttrain-mlogloss:0.5809\ttest-mlogloss:0.730301\n",
      "[1580]\ttrain-mlogloss:0.580213\ttest-mlogloss:0.730291\n",
      "[1590]\ttrain-mlogloss:0.579491\ttest-mlogloss:0.730197\n",
      "[1600]\ttrain-mlogloss:0.578894\ttest-mlogloss:0.730221\n",
      "[1610]\ttrain-mlogloss:0.578302\ttest-mlogloss:0.730214\n",
      "[1620]\ttrain-mlogloss:0.577742\ttest-mlogloss:0.730204\n",
      "[1630]\ttrain-mlogloss:0.577147\ttest-mlogloss:0.730179\n",
      "[1640]\ttrain-mlogloss:0.576636\ttest-mlogloss:0.730206\n",
      "[1650]\ttrain-mlogloss:0.575917\ttest-mlogloss:0.730219\n",
      "[1660]\ttrain-mlogloss:0.575337\ttest-mlogloss:0.730209\n",
      "Stopping. Best iteration:\n",
      "[1631]\ttrain-mlogloss:0.577093\ttest-mlogloss:0.730178\n",
      "\n",
      "[0]\ttrain-mlogloss:1.09496\ttest-mlogloss:1.09514\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[10]\ttrain-mlogloss:1.06056\ttest-mlogloss:1.06221\n",
      "[20]\ttrain-mlogloss:1.03058\ttest-mlogloss:1.03383\n",
      "[30]\ttrain-mlogloss:1.00379\ttest-mlogloss:1.00853\n",
      "[40]\ttrain-mlogloss:0.97998\ttest-mlogloss:0.986193\n",
      "[50]\ttrain-mlogloss:0.958293\ttest-mlogloss:0.966109\n",
      "[60]\ttrain-mlogloss:0.939315\ttest-mlogloss:0.948579\n",
      "[70]\ttrain-mlogloss:0.921247\ttest-mlogloss:0.932198\n",
      "[80]\ttrain-mlogloss:0.905053\ttest-mlogloss:0.917483\n",
      "[90]\ttrain-mlogloss:0.89049\ttest-mlogloss:0.904299\n",
      "[100]\ttrain-mlogloss:0.877057\ttest-mlogloss:0.892105\n",
      "[110]\ttrain-mlogloss:0.864752\ttest-mlogloss:0.881231\n",
      "[120]\ttrain-mlogloss:0.853845\ttest-mlogloss:0.871807\n",
      "[130]\ttrain-mlogloss:0.843356\ttest-mlogloss:0.86284\n",
      "[140]\ttrain-mlogloss:0.833658\ttest-mlogloss:0.854557\n",
      "[150]\ttrain-mlogloss:0.824707\ttest-mlogloss:0.847015\n",
      "[160]\ttrain-mlogloss:0.81625\ttest-mlogloss:0.84002\n",
      "[170]\ttrain-mlogloss:0.808503\ttest-mlogloss:0.833657\n",
      "[180]\ttrain-mlogloss:0.801318\ttest-mlogloss:0.827905\n",
      "[190]\ttrain-mlogloss:0.794196\ttest-mlogloss:0.82225\n",
      "[200]\ttrain-mlogloss:0.787877\ttest-mlogloss:0.817303\n",
      "[210]\ttrain-mlogloss:0.781874\ttest-mlogloss:0.812738\n",
      "[220]\ttrain-mlogloss:0.776179\ttest-mlogloss:0.808403\n",
      "[230]\ttrain-mlogloss:0.770837\ttest-mlogloss:0.804325\n",
      "[240]\ttrain-mlogloss:0.765869\ttest-mlogloss:0.800665\n",
      "[250]\ttrain-mlogloss:0.76072\ttest-mlogloss:0.797062\n",
      "[260]\ttrain-mlogloss:0.756149\ttest-mlogloss:0.793857\n",
      "[270]\ttrain-mlogloss:0.751837\ttest-mlogloss:0.790883\n",
      "[280]\ttrain-mlogloss:0.74757\ttest-mlogloss:0.78801\n",
      "[290]\ttrain-mlogloss:0.743743\ttest-mlogloss:0.78535\n",
      "[300]\ttrain-mlogloss:0.739836\ttest-mlogloss:0.782711\n",
      "[310]\ttrain-mlogloss:0.736188\ttest-mlogloss:0.780417\n",
      "[320]\ttrain-mlogloss:0.732709\ttest-mlogloss:0.778212\n",
      "[330]\ttrain-mlogloss:0.729047\ttest-mlogloss:0.775854\n",
      "[340]\ttrain-mlogloss:0.725865\ttest-mlogloss:0.773875\n",
      "[350]\ttrain-mlogloss:0.722475\ttest-mlogloss:0.771811\n",
      "[360]\ttrain-mlogloss:0.719419\ttest-mlogloss:0.769962\n",
      "[370]\ttrain-mlogloss:0.716378\ttest-mlogloss:0.768307\n",
      "[380]\ttrain-mlogloss:0.713685\ttest-mlogloss:0.766742\n",
      "[390]\ttrain-mlogloss:0.710832\ttest-mlogloss:0.765125\n",
      "[400]\ttrain-mlogloss:0.708091\ttest-mlogloss:0.763608\n",
      "[410]\ttrain-mlogloss:0.705495\ttest-mlogloss:0.762167\n",
      "[420]\ttrain-mlogloss:0.702988\ttest-mlogloss:0.760852\n",
      "[430]\ttrain-mlogloss:0.700603\ttest-mlogloss:0.759665\n",
      "[440]\ttrain-mlogloss:0.698182\ttest-mlogloss:0.758329\n",
      "[450]\ttrain-mlogloss:0.695899\ttest-mlogloss:0.757222\n",
      "[460]\ttrain-mlogloss:0.693784\ttest-mlogloss:0.75614\n",
      "[470]\ttrain-mlogloss:0.691716\ttest-mlogloss:0.755129\n",
      "[480]\ttrain-mlogloss:0.689592\ttest-mlogloss:0.753973\n",
      "[490]\ttrain-mlogloss:0.687425\ttest-mlogloss:0.752889\n",
      "[500]\ttrain-mlogloss:0.685521\ttest-mlogloss:0.752076\n",
      "[510]\ttrain-mlogloss:0.683349\ttest-mlogloss:0.751024\n",
      "[520]\ttrain-mlogloss:0.681585\ttest-mlogloss:0.750247\n",
      "[530]\ttrain-mlogloss:0.679888\ttest-mlogloss:0.749484\n",
      "[540]\ttrain-mlogloss:0.678208\ttest-mlogloss:0.748789\n",
      "[550]\ttrain-mlogloss:0.676428\ttest-mlogloss:0.748043\n",
      "[560]\ttrain-mlogloss:0.674633\ttest-mlogloss:0.747226\n",
      "[570]\ttrain-mlogloss:0.672989\ttest-mlogloss:0.746533\n",
      "[580]\ttrain-mlogloss:0.671396\ttest-mlogloss:0.746026\n",
      "[590]\ttrain-mlogloss:0.669713\ttest-mlogloss:0.745339\n",
      "[600]\ttrain-mlogloss:0.668362\ttest-mlogloss:0.74485\n",
      "[610]\ttrain-mlogloss:0.666767\ttest-mlogloss:0.744307\n",
      "[620]\ttrain-mlogloss:0.665095\ttest-mlogloss:0.743641\n",
      "[630]\ttrain-mlogloss:0.663733\ttest-mlogloss:0.743159\n",
      "[640]\ttrain-mlogloss:0.662419\ttest-mlogloss:0.742678\n",
      "[650]\ttrain-mlogloss:0.661016\ttest-mlogloss:0.742165\n",
      "[660]\ttrain-mlogloss:0.659517\ttest-mlogloss:0.7416\n",
      "[670]\ttrain-mlogloss:0.657799\ttest-mlogloss:0.740907\n",
      "[680]\ttrain-mlogloss:0.656323\ttest-mlogloss:0.740266\n",
      "[690]\ttrain-mlogloss:0.655121\ttest-mlogloss:0.739832\n",
      "[700]\ttrain-mlogloss:0.653783\ttest-mlogloss:0.739308\n",
      "[710]\ttrain-mlogloss:0.652579\ttest-mlogloss:0.738952\n",
      "[720]\ttrain-mlogloss:0.651312\ttest-mlogloss:0.73853\n",
      "[730]\ttrain-mlogloss:0.65013\ttest-mlogloss:0.738138\n",
      "[740]\ttrain-mlogloss:0.649098\ttest-mlogloss:0.737815\n",
      "[750]\ttrain-mlogloss:0.648105\ttest-mlogloss:0.737579\n",
      "[760]\ttrain-mlogloss:0.647043\ttest-mlogloss:0.737306\n",
      "[770]\ttrain-mlogloss:0.645911\ttest-mlogloss:0.737004\n",
      "[780]\ttrain-mlogloss:0.644832\ttest-mlogloss:0.73671\n",
      "[790]\ttrain-mlogloss:0.643685\ttest-mlogloss:0.736324\n",
      "[800]\ttrain-mlogloss:0.642693\ttest-mlogloss:0.736023\n",
      "[810]\ttrain-mlogloss:0.641498\ttest-mlogloss:0.735658\n",
      "[820]\ttrain-mlogloss:0.640416\ttest-mlogloss:0.735373\n",
      "[830]\ttrain-mlogloss:0.639228\ttest-mlogloss:0.73503\n",
      "[840]\ttrain-mlogloss:0.638176\ttest-mlogloss:0.734762\n",
      "[850]\ttrain-mlogloss:0.637078\ttest-mlogloss:0.734407\n",
      "[860]\ttrain-mlogloss:0.636081\ttest-mlogloss:0.734179\n",
      "[870]\ttrain-mlogloss:0.635063\ttest-mlogloss:0.733974\n",
      "[880]\ttrain-mlogloss:0.634149\ttest-mlogloss:0.733795\n",
      "[890]\ttrain-mlogloss:0.633124\ttest-mlogloss:0.73357\n",
      "[900]\ttrain-mlogloss:0.632121\ttest-mlogloss:0.733291\n",
      "[910]\ttrain-mlogloss:0.631166\ttest-mlogloss:0.733065\n",
      "[920]\ttrain-mlogloss:0.630217\ttest-mlogloss:0.732814\n",
      "[930]\ttrain-mlogloss:0.629153\ttest-mlogloss:0.732656\n",
      "[940]\ttrain-mlogloss:0.628162\ttest-mlogloss:0.732416\n",
      "[950]\ttrain-mlogloss:0.627317\ttest-mlogloss:0.732255\n",
      "[960]\ttrain-mlogloss:0.626496\ttest-mlogloss:0.732148\n",
      "[970]\ttrain-mlogloss:0.625505\ttest-mlogloss:0.731943\n",
      "[980]\ttrain-mlogloss:0.62455\ttest-mlogloss:0.731677\n",
      "[990]\ttrain-mlogloss:0.623593\ttest-mlogloss:0.731416\n",
      "[1000]\ttrain-mlogloss:0.622579\ttest-mlogloss:0.731211\n",
      "[1010]\ttrain-mlogloss:0.621666\ttest-mlogloss:0.731043\n",
      "[1020]\ttrain-mlogloss:0.620772\ttest-mlogloss:0.730845\n",
      "[1030]\ttrain-mlogloss:0.619906\ttest-mlogloss:0.730676\n",
      "[1040]\ttrain-mlogloss:0.619087\ttest-mlogloss:0.730509\n",
      "[1050]\ttrain-mlogloss:0.618168\ttest-mlogloss:0.730312\n",
      "[1060]\ttrain-mlogloss:0.617354\ttest-mlogloss:0.730185\n",
      "[1070]\ttrain-mlogloss:0.616497\ttest-mlogloss:0.729994\n",
      "[1080]\ttrain-mlogloss:0.615772\ttest-mlogloss:0.729866\n",
      "[1090]\ttrain-mlogloss:0.614945\ttest-mlogloss:0.729738\n",
      "[1100]\ttrain-mlogloss:0.614088\ttest-mlogloss:0.729598\n",
      "[1110]\ttrain-mlogloss:0.613203\ttest-mlogloss:0.729439\n",
      "[1120]\ttrain-mlogloss:0.612479\ttest-mlogloss:0.729291\n",
      "[1130]\ttrain-mlogloss:0.611678\ttest-mlogloss:0.729166\n",
      "[1140]\ttrain-mlogloss:0.610965\ttest-mlogloss:0.729025\n",
      "[1150]\ttrain-mlogloss:0.610144\ttest-mlogloss:0.728901\n",
      "[1160]\ttrain-mlogloss:0.609469\ttest-mlogloss:0.728808\n",
      "[1170]\ttrain-mlogloss:0.608633\ttest-mlogloss:0.728633\n",
      "[1180]\ttrain-mlogloss:0.607869\ttest-mlogloss:0.728488\n",
      "[1190]\ttrain-mlogloss:0.606997\ttest-mlogloss:0.728405\n",
      "[1200]\ttrain-mlogloss:0.606241\ttest-mlogloss:0.728337\n",
      "[1210]\ttrain-mlogloss:0.605493\ttest-mlogloss:0.728286\n",
      "[1220]\ttrain-mlogloss:0.604764\ttest-mlogloss:0.728246\n",
      "[1230]\ttrain-mlogloss:0.604044\ttest-mlogloss:0.728152\n",
      "[1240]\ttrain-mlogloss:0.60334\ttest-mlogloss:0.72803\n",
      "[1250]\ttrain-mlogloss:0.602603\ttest-mlogloss:0.727918\n",
      "[1260]\ttrain-mlogloss:0.601848\ttest-mlogloss:0.727869\n",
      "[1270]\ttrain-mlogloss:0.601059\ttest-mlogloss:0.727742\n",
      "[1280]\ttrain-mlogloss:0.600266\ttest-mlogloss:0.727584\n",
      "[1290]\ttrain-mlogloss:0.599476\ttest-mlogloss:0.727457\n",
      "[1300]\ttrain-mlogloss:0.598896\ttest-mlogloss:0.727395\n",
      "[1310]\ttrain-mlogloss:0.598152\ttest-mlogloss:0.727286\n",
      "[1320]\ttrain-mlogloss:0.597402\ttest-mlogloss:0.727242\n",
      "[1330]\ttrain-mlogloss:0.596689\ttest-mlogloss:0.727178\n",
      "[1340]\ttrain-mlogloss:0.596069\ttest-mlogloss:0.727081\n",
      "[1350]\ttrain-mlogloss:0.595315\ttest-mlogloss:0.72701\n",
      "[1360]\ttrain-mlogloss:0.594527\ttest-mlogloss:0.726885\n",
      "[1370]\ttrain-mlogloss:0.593855\ttest-mlogloss:0.726767\n",
      "[1380]\ttrain-mlogloss:0.593177\ttest-mlogloss:0.726628\n",
      "[1390]\ttrain-mlogloss:0.592514\ttest-mlogloss:0.726546\n",
      "[1400]\ttrain-mlogloss:0.591888\ttest-mlogloss:0.726434\n",
      "[1410]\ttrain-mlogloss:0.591237\ttest-mlogloss:0.726361\n",
      "[1420]\ttrain-mlogloss:0.590443\ttest-mlogloss:0.726186\n",
      "[1430]\ttrain-mlogloss:0.589845\ttest-mlogloss:0.726148\n",
      "[1440]\ttrain-mlogloss:0.5891\ttest-mlogloss:0.726082\n",
      "[1450]\ttrain-mlogloss:0.588403\ttest-mlogloss:0.725977\n",
      "[1460]\ttrain-mlogloss:0.587689\ttest-mlogloss:0.725861\n",
      "[1470]\ttrain-mlogloss:0.587133\ttest-mlogloss:0.725809\n",
      "[1480]\ttrain-mlogloss:0.586445\ttest-mlogloss:0.725711\n",
      "[1490]\ttrain-mlogloss:0.585724\ttest-mlogloss:0.725617\n",
      "[1500]\ttrain-mlogloss:0.585158\ttest-mlogloss:0.725589\n",
      "[1510]\ttrain-mlogloss:0.584582\ttest-mlogloss:0.725537\n",
      "[1520]\ttrain-mlogloss:0.583966\ttest-mlogloss:0.725478\n",
      "[1530]\ttrain-mlogloss:0.583352\ttest-mlogloss:0.725445\n",
      "[1540]\ttrain-mlogloss:0.582758\ttest-mlogloss:0.725354\n",
      "[1550]\ttrain-mlogloss:0.582181\ttest-mlogloss:0.725331\n",
      "[1560]\ttrain-mlogloss:0.581492\ttest-mlogloss:0.725321\n",
      "[1570]\ttrain-mlogloss:0.580875\ttest-mlogloss:0.72533\n",
      "[1580]\ttrain-mlogloss:0.580274\ttest-mlogloss:0.725311\n",
      "[1590]\ttrain-mlogloss:0.57956\ttest-mlogloss:0.725213\n",
      "[1600]\ttrain-mlogloss:0.57899\ttest-mlogloss:0.725165\n",
      "[1610]\ttrain-mlogloss:0.578327\ttest-mlogloss:0.725065\n",
      "[1620]\ttrain-mlogloss:0.577813\ttest-mlogloss:0.725017\n",
      "[1630]\ttrain-mlogloss:0.577187\ttest-mlogloss:0.72499\n",
      "[1640]\ttrain-mlogloss:0.576649\ttest-mlogloss:0.724939\n",
      "[1650]\ttrain-mlogloss:0.576021\ttest-mlogloss:0.724882\n",
      "[1660]\ttrain-mlogloss:0.575412\ttest-mlogloss:0.724759\n",
      "[1670]\ttrain-mlogloss:0.574857\ttest-mlogloss:0.724654\n",
      "[1680]\ttrain-mlogloss:0.574337\ttest-mlogloss:0.724661\n",
      "[1690]\ttrain-mlogloss:0.573739\ttest-mlogloss:0.724583\n",
      "[1700]\ttrain-mlogloss:0.573132\ttest-mlogloss:0.724488\n",
      "[1710]\ttrain-mlogloss:0.57248\ttest-mlogloss:0.724397\n",
      "[1720]\ttrain-mlogloss:0.571936\ttest-mlogloss:0.724342\n",
      "[1730]\ttrain-mlogloss:0.571414\ttest-mlogloss:0.724276\n",
      "[1740]\ttrain-mlogloss:0.570818\ttest-mlogloss:0.724249\n",
      "[1750]\ttrain-mlogloss:0.570134\ttest-mlogloss:0.724203\n",
      "[1760]\ttrain-mlogloss:0.56944\ttest-mlogloss:0.724189\n",
      "[1770]\ttrain-mlogloss:0.568838\ttest-mlogloss:0.724159\n",
      "[1780]\ttrain-mlogloss:0.568271\ttest-mlogloss:0.724076\n",
      "[1790]\ttrain-mlogloss:0.567717\ttest-mlogloss:0.724002\n",
      "[1800]\ttrain-mlogloss:0.567147\ttest-mlogloss:0.723976\n",
      "[1810]\ttrain-mlogloss:0.566595\ttest-mlogloss:0.723922\n",
      "[1820]\ttrain-mlogloss:0.566063\ttest-mlogloss:0.72387\n",
      "[1830]\ttrain-mlogloss:0.565536\ttest-mlogloss:0.723865\n",
      "[1840]\ttrain-mlogloss:0.564858\ttest-mlogloss:0.723786\n",
      "[1850]\ttrain-mlogloss:0.564281\ttest-mlogloss:0.723771\n",
      "[1860]\ttrain-mlogloss:0.563691\ttest-mlogloss:0.723721\n",
      "[1870]\ttrain-mlogloss:0.563187\ttest-mlogloss:0.723685\n",
      "[1880]\ttrain-mlogloss:0.562561\ttest-mlogloss:0.72367\n",
      "[1890]\ttrain-mlogloss:0.562075\ttest-mlogloss:0.723671\n",
      "[1900]\ttrain-mlogloss:0.561501\ttest-mlogloss:0.723611\n",
      "[1910]\ttrain-mlogloss:0.560926\ttest-mlogloss:0.723611\n",
      "[1920]\ttrain-mlogloss:0.560412\ttest-mlogloss:0.723615\n",
      "[1930]\ttrain-mlogloss:0.559887\ttest-mlogloss:0.723578\n",
      "[1940]\ttrain-mlogloss:0.559343\ttest-mlogloss:0.723544\n",
      "[1950]\ttrain-mlogloss:0.558732\ttest-mlogloss:0.723499\n",
      "[1960]\ttrain-mlogloss:0.558208\ttest-mlogloss:0.723463\n",
      "[1970]\ttrain-mlogloss:0.5576\ttest-mlogloss:0.723405\n",
      "[1980]\ttrain-mlogloss:0.557042\ttest-mlogloss:0.723348\n",
      "[1990]\ttrain-mlogloss:0.556301\ttest-mlogloss:0.723278\n",
      "[2000]\ttrain-mlogloss:0.555875\ttest-mlogloss:0.723275\n",
      "[2010]\ttrain-mlogloss:0.555369\ttest-mlogloss:0.723222\n",
      "[2020]\ttrain-mlogloss:0.55488\ttest-mlogloss:0.723136\n",
      "[2030]\ttrain-mlogloss:0.55435\ttest-mlogloss:0.723105\n",
      "[2040]\ttrain-mlogloss:0.553852\ttest-mlogloss:0.723057\n",
      "[2050]\ttrain-mlogloss:0.55334\ttest-mlogloss:0.723046\n",
      "[2060]\ttrain-mlogloss:0.552779\ttest-mlogloss:0.722955\n",
      "[2070]\ttrain-mlogloss:0.552113\ttest-mlogloss:0.722912\n",
      "[2080]\ttrain-mlogloss:0.551618\ttest-mlogloss:0.722912\n",
      "[2090]\ttrain-mlogloss:0.551091\ttest-mlogloss:0.722872\n",
      "[2100]\ttrain-mlogloss:0.550526\ttest-mlogloss:0.722891\n",
      "[2110]\ttrain-mlogloss:0.549952\ttest-mlogloss:0.722839\n",
      "[2120]\ttrain-mlogloss:0.549428\ttest-mlogloss:0.722781\n",
      "[2130]\ttrain-mlogloss:0.548856\ttest-mlogloss:0.722779\n",
      "[2140]\ttrain-mlogloss:0.54824\ttest-mlogloss:0.722729\n",
      "[2150]\ttrain-mlogloss:0.547729\ttest-mlogloss:0.722746\n",
      "[2160]\ttrain-mlogloss:0.547162\ttest-mlogloss:0.722687\n",
      "[2170]\ttrain-mlogloss:0.546593\ttest-mlogloss:0.722645\n",
      "[2180]\ttrain-mlogloss:0.546097\ttest-mlogloss:0.72261\n",
      "[2190]\ttrain-mlogloss:0.545586\ttest-mlogloss:0.72258\n",
      "[2200]\ttrain-mlogloss:0.544983\ttest-mlogloss:0.722547\n",
      "[2210]\ttrain-mlogloss:0.544552\ttest-mlogloss:0.722521\n",
      "[2220]\ttrain-mlogloss:0.543921\ttest-mlogloss:0.722478\n",
      "[2230]\ttrain-mlogloss:0.543363\ttest-mlogloss:0.722512\n",
      "[2240]\ttrain-mlogloss:0.542826\ttest-mlogloss:0.722508\n",
      "[2250]\ttrain-mlogloss:0.542258\ttest-mlogloss:0.7225\n",
      "Stopping. Best iteration:\n",
      "[2222]\ttrain-mlogloss:0.543794\ttest-mlogloss:0.722466\n",
      "\n",
      "[0]\ttrain-mlogloss:1.09493\ttest-mlogloss:1.09507\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[10]\ttrain-mlogloss:1.06039\ttest-mlogloss:1.06188\n",
      "[20]\ttrain-mlogloss:1.03066\ttest-mlogloss:1.03346\n",
      "[30]\ttrain-mlogloss:1.00382\ttest-mlogloss:1.0082\n",
      "[40]\ttrain-mlogloss:0.979861\ttest-mlogloss:0.985626\n",
      "[50]\ttrain-mlogloss:0.958162\ttest-mlogloss:0.965435\n",
      "[60]\ttrain-mlogloss:0.939023\ttest-mlogloss:0.947645\n",
      "[70]\ttrain-mlogloss:0.920744\ttest-mlogloss:0.930983\n",
      "[80]\ttrain-mlogloss:0.904615\ttest-mlogloss:0.916315\n",
      "[90]\ttrain-mlogloss:0.890062\ttest-mlogloss:0.90322\n",
      "[100]\ttrain-mlogloss:0.876818\ttest-mlogloss:0.891366\n",
      "[110]\ttrain-mlogloss:0.864259\ttest-mlogloss:0.880314\n",
      "[120]\ttrain-mlogloss:0.853212\ttest-mlogloss:0.87077\n",
      "[130]\ttrain-mlogloss:0.84255\ttest-mlogloss:0.861605\n",
      "[140]\ttrain-mlogloss:0.832889\ttest-mlogloss:0.853381\n",
      "[150]\ttrain-mlogloss:0.823947\ttest-mlogloss:0.845923\n",
      "[160]\ttrain-mlogloss:0.815477\ttest-mlogloss:0.839026\n",
      "[170]\ttrain-mlogloss:0.807774\ttest-mlogloss:0.832829\n",
      "[180]\ttrain-mlogloss:0.800497\ttest-mlogloss:0.827096\n",
      "[190]\ttrain-mlogloss:0.793628\ttest-mlogloss:0.821834\n",
      "[200]\ttrain-mlogloss:0.787244\ttest-mlogloss:0.816919\n",
      "[210]\ttrain-mlogloss:0.78128\ttest-mlogloss:0.81261\n",
      "[220]\ttrain-mlogloss:0.775541\ttest-mlogloss:0.808385\n",
      "[230]\ttrain-mlogloss:0.770098\ttest-mlogloss:0.804551\n",
      "[240]\ttrain-mlogloss:0.765254\ttest-mlogloss:0.801137\n",
      "[250]\ttrain-mlogloss:0.760284\ttest-mlogloss:0.797671\n",
      "[260]\ttrain-mlogloss:0.755611\ttest-mlogloss:0.794506\n",
      "[270]\ttrain-mlogloss:0.751104\ttest-mlogloss:0.791384\n",
      "[280]\ttrain-mlogloss:0.746661\ttest-mlogloss:0.788511\n",
      "[290]\ttrain-mlogloss:0.742763\ttest-mlogloss:0.785994\n",
      "[300]\ttrain-mlogloss:0.738742\ttest-mlogloss:0.783438\n",
      "[310]\ttrain-mlogloss:0.734942\ttest-mlogloss:0.781218\n",
      "[320]\ttrain-mlogloss:0.73155\ttest-mlogloss:0.77926\n",
      "[330]\ttrain-mlogloss:0.728233\ttest-mlogloss:0.777364\n",
      "[340]\ttrain-mlogloss:0.724981\ttest-mlogloss:0.77555\n",
      "[350]\ttrain-mlogloss:0.721776\ttest-mlogloss:0.77377\n",
      "[360]\ttrain-mlogloss:0.718872\ttest-mlogloss:0.77206\n",
      "[370]\ttrain-mlogloss:0.71611\ttest-mlogloss:0.770577\n",
      "[380]\ttrain-mlogloss:0.713108\ttest-mlogloss:0.769027\n",
      "[390]\ttrain-mlogloss:0.710354\ttest-mlogloss:0.767576\n",
      "[400]\ttrain-mlogloss:0.707589\ttest-mlogloss:0.766064\n",
      "[410]\ttrain-mlogloss:0.704785\ttest-mlogloss:0.764652\n",
      "[420]\ttrain-mlogloss:0.70236\ttest-mlogloss:0.763473\n",
      "[430]\ttrain-mlogloss:0.699979\ttest-mlogloss:0.762413\n",
      "[440]\ttrain-mlogloss:0.697664\ttest-mlogloss:0.761372\n",
      "[450]\ttrain-mlogloss:0.695375\ttest-mlogloss:0.760285\n",
      "[460]\ttrain-mlogloss:0.692964\ttest-mlogloss:0.7591\n",
      "[470]\ttrain-mlogloss:0.69097\ttest-mlogloss:0.758211\n",
      "[480]\ttrain-mlogloss:0.688783\ttest-mlogloss:0.757187\n",
      "[490]\ttrain-mlogloss:0.686743\ttest-mlogloss:0.75626\n",
      "[500]\ttrain-mlogloss:0.684661\ttest-mlogloss:0.755357\n",
      "[510]\ttrain-mlogloss:0.682561\ttest-mlogloss:0.754487\n",
      "[520]\ttrain-mlogloss:0.680594\ttest-mlogloss:0.75371\n",
      "[530]\ttrain-mlogloss:0.678864\ttest-mlogloss:0.753112\n",
      "[540]\ttrain-mlogloss:0.677078\ttest-mlogloss:0.752408\n",
      "[550]\ttrain-mlogloss:0.675402\ttest-mlogloss:0.751732\n",
      "[560]\ttrain-mlogloss:0.673658\ttest-mlogloss:0.751134\n",
      "[570]\ttrain-mlogloss:0.671905\ttest-mlogloss:0.750543\n",
      "[580]\ttrain-mlogloss:0.670295\ttest-mlogloss:0.749933\n",
      "[590]\ttrain-mlogloss:0.668762\ttest-mlogloss:0.749366\n",
      "[600]\ttrain-mlogloss:0.667174\ttest-mlogloss:0.748854\n",
      "[610]\ttrain-mlogloss:0.665655\ttest-mlogloss:0.748366\n",
      "[620]\ttrain-mlogloss:0.664081\ttest-mlogloss:0.747781\n",
      "[630]\ttrain-mlogloss:0.662481\ttest-mlogloss:0.74719\n",
      "[640]\ttrain-mlogloss:0.660979\ttest-mlogloss:0.746817\n",
      "[650]\ttrain-mlogloss:0.659719\ttest-mlogloss:0.746476\n",
      "[660]\ttrain-mlogloss:0.658224\ttest-mlogloss:0.745852\n",
      "[670]\ttrain-mlogloss:0.65677\ttest-mlogloss:0.745404\n",
      "[680]\ttrain-mlogloss:0.655256\ttest-mlogloss:0.744928\n",
      "[690]\ttrain-mlogloss:0.653777\ttest-mlogloss:0.744506\n",
      "[700]\ttrain-mlogloss:0.652541\ttest-mlogloss:0.744228\n",
      "[710]\ttrain-mlogloss:0.651206\ttest-mlogloss:0.743831\n",
      "[720]\ttrain-mlogloss:0.649782\ttest-mlogloss:0.743413\n",
      "[730]\ttrain-mlogloss:0.648474\ttest-mlogloss:0.743046\n",
      "[740]\ttrain-mlogloss:0.647322\ttest-mlogloss:0.742743\n",
      "[750]\ttrain-mlogloss:0.646263\ttest-mlogloss:0.742468\n",
      "[760]\ttrain-mlogloss:0.645079\ttest-mlogloss:0.742243\n",
      "[770]\ttrain-mlogloss:0.643875\ttest-mlogloss:0.741911\n",
      "[780]\ttrain-mlogloss:0.642762\ttest-mlogloss:0.741693\n",
      "[790]\ttrain-mlogloss:0.641677\ttest-mlogloss:0.741488\n",
      "[800]\ttrain-mlogloss:0.640554\ttest-mlogloss:0.741076\n",
      "[810]\ttrain-mlogloss:0.639425\ttest-mlogloss:0.740881\n",
      "[820]\ttrain-mlogloss:0.638263\ttest-mlogloss:0.74059\n",
      "[830]\ttrain-mlogloss:0.637204\ttest-mlogloss:0.740432\n",
      "[840]\ttrain-mlogloss:0.636119\ttest-mlogloss:0.740121\n",
      "[850]\ttrain-mlogloss:0.635085\ttest-mlogloss:0.739914\n",
      "[860]\ttrain-mlogloss:0.634063\ttest-mlogloss:0.739738\n",
      "[870]\ttrain-mlogloss:0.633001\ttest-mlogloss:0.739569\n",
      "[880]\ttrain-mlogloss:0.631844\ttest-mlogloss:0.739435\n",
      "[890]\ttrain-mlogloss:0.63084\ttest-mlogloss:0.739306\n",
      "[900]\ttrain-mlogloss:0.630046\ttest-mlogloss:0.73919\n",
      "[910]\ttrain-mlogloss:0.629001\ttest-mlogloss:0.739028\n",
      "[920]\ttrain-mlogloss:0.62815\ttest-mlogloss:0.738948\n",
      "[930]\ttrain-mlogloss:0.627334\ttest-mlogloss:0.738796\n",
      "[940]\ttrain-mlogloss:0.626309\ttest-mlogloss:0.73862\n",
      "[950]\ttrain-mlogloss:0.625468\ttest-mlogloss:0.7385\n",
      "[960]\ttrain-mlogloss:0.624394\ttest-mlogloss:0.738351\n",
      "[970]\ttrain-mlogloss:0.623401\ttest-mlogloss:0.738209\n",
      "[980]\ttrain-mlogloss:0.622337\ttest-mlogloss:0.738052\n",
      "[990]\ttrain-mlogloss:0.621228\ttest-mlogloss:0.737882\n",
      "[1000]\ttrain-mlogloss:0.620395\ttest-mlogloss:0.737745\n",
      "[1010]\ttrain-mlogloss:0.619544\ttest-mlogloss:0.737642\n",
      "[1020]\ttrain-mlogloss:0.61868\ttest-mlogloss:0.737479\n",
      "[1030]\ttrain-mlogloss:0.61786\ttest-mlogloss:0.737397\n",
      "[1040]\ttrain-mlogloss:0.616929\ttest-mlogloss:0.737269\n",
      "[1050]\ttrain-mlogloss:0.616077\ttest-mlogloss:0.737137\n",
      "[1060]\ttrain-mlogloss:0.61524\ttest-mlogloss:0.736994\n",
      "[1070]\ttrain-mlogloss:0.614263\ttest-mlogloss:0.736812\n",
      "[1080]\ttrain-mlogloss:0.613463\ttest-mlogloss:0.736738\n",
      "[1090]\ttrain-mlogloss:0.612499\ttest-mlogloss:0.736543\n",
      "[1100]\ttrain-mlogloss:0.611695\ttest-mlogloss:0.736514\n",
      "[1110]\ttrain-mlogloss:0.610899\ttest-mlogloss:0.736418\n",
      "[1120]\ttrain-mlogloss:0.610001\ttest-mlogloss:0.736271\n",
      "[1130]\ttrain-mlogloss:0.609214\ttest-mlogloss:0.736225\n",
      "[1140]\ttrain-mlogloss:0.608466\ttest-mlogloss:0.736161\n",
      "[1150]\ttrain-mlogloss:0.60766\ttest-mlogloss:0.736059\n",
      "[1160]\ttrain-mlogloss:0.606912\ttest-mlogloss:0.735974\n",
      "[1170]\ttrain-mlogloss:0.606135\ttest-mlogloss:0.735916\n",
      "[1180]\ttrain-mlogloss:0.605442\ttest-mlogloss:0.735841\n",
      "[1190]\ttrain-mlogloss:0.604662\ttest-mlogloss:0.73579\n",
      "[1200]\ttrain-mlogloss:0.603903\ttest-mlogloss:0.735703\n",
      "[1210]\ttrain-mlogloss:0.603088\ttest-mlogloss:0.735614\n",
      "[1220]\ttrain-mlogloss:0.602399\ttest-mlogloss:0.73556\n",
      "[1230]\ttrain-mlogloss:0.601598\ttest-mlogloss:0.73549\n",
      "[1240]\ttrain-mlogloss:0.600855\ttest-mlogloss:0.735364\n",
      "[1250]\ttrain-mlogloss:0.599959\ttest-mlogloss:0.735267\n",
      "[1260]\ttrain-mlogloss:0.599335\ttest-mlogloss:0.735225\n",
      "[1270]\ttrain-mlogloss:0.598603\ttest-mlogloss:0.735186\n",
      "[1280]\ttrain-mlogloss:0.597733\ttest-mlogloss:0.735097\n",
      "[1290]\ttrain-mlogloss:0.596901\ttest-mlogloss:0.735036\n",
      "[1300]\ttrain-mlogloss:0.5962\ttest-mlogloss:0.734942\n",
      "[1310]\ttrain-mlogloss:0.59552\ttest-mlogloss:0.73492\n",
      "[1320]\ttrain-mlogloss:0.59481\ttest-mlogloss:0.73495\n",
      "[1330]\ttrain-mlogloss:0.594049\ttest-mlogloss:0.734847\n",
      "[1340]\ttrain-mlogloss:0.593291\ttest-mlogloss:0.734805\n",
      "[1350]\ttrain-mlogloss:0.592498\ttest-mlogloss:0.734727\n",
      "[1360]\ttrain-mlogloss:0.591877\ttest-mlogloss:0.734689\n",
      "[1370]\ttrain-mlogloss:0.591117\ttest-mlogloss:0.734609\n",
      "[1380]\ttrain-mlogloss:0.590403\ttest-mlogloss:0.734555\n",
      "[1390]\ttrain-mlogloss:0.589713\ttest-mlogloss:0.734495\n",
      "[1400]\ttrain-mlogloss:0.588928\ttest-mlogloss:0.73445\n",
      "[1410]\ttrain-mlogloss:0.588332\ttest-mlogloss:0.734413\n",
      "[1420]\ttrain-mlogloss:0.587628\ttest-mlogloss:0.734395\n",
      "[1430]\ttrain-mlogloss:0.58685\ttest-mlogloss:0.734368\n",
      "[1440]\ttrain-mlogloss:0.586173\ttest-mlogloss:0.734317\n",
      "[1450]\ttrain-mlogloss:0.58551\ttest-mlogloss:0.734273\n",
      "[1460]\ttrain-mlogloss:0.584891\ttest-mlogloss:0.734233\n",
      "[1470]\ttrain-mlogloss:0.584245\ttest-mlogloss:0.734227\n",
      "[1480]\ttrain-mlogloss:0.583659\ttest-mlogloss:0.734191\n",
      "[1490]\ttrain-mlogloss:0.582928\ttest-mlogloss:0.734207\n",
      "[1500]\ttrain-mlogloss:0.582285\ttest-mlogloss:0.734217\n",
      "[1510]\ttrain-mlogloss:0.581691\ttest-mlogloss:0.734191\n",
      "[1520]\ttrain-mlogloss:0.581059\ttest-mlogloss:0.734097\n",
      "[1530]\ttrain-mlogloss:0.5804\ttest-mlogloss:0.734058\n",
      "[1540]\ttrain-mlogloss:0.579757\ttest-mlogloss:0.734062\n",
      "[1550]\ttrain-mlogloss:0.57922\ttest-mlogloss:0.73409\n",
      "Stopping. Best iteration:\n",
      "[1526]\ttrain-mlogloss:0.580639\ttest-mlogloss:0.734048\n",
      "\n",
      "[0]\ttrain-mlogloss:1.09498\ttest-mlogloss:1.09515\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[10]\ttrain-mlogloss:1.06047\ttest-mlogloss:1.0619\n",
      "[20]\ttrain-mlogloss:1.03041\ttest-mlogloss:1.03309\n",
      "[30]\ttrain-mlogloss:1.00385\ttest-mlogloss:1.00776\n",
      "[40]\ttrain-mlogloss:0.979892\ttest-mlogloss:0.984895\n",
      "[50]\ttrain-mlogloss:0.958388\ttest-mlogloss:0.964858\n",
      "[60]\ttrain-mlogloss:0.939304\ttest-mlogloss:0.947035\n",
      "[70]\ttrain-mlogloss:0.921099\ttest-mlogloss:0.930211\n",
      "[80]\ttrain-mlogloss:0.905026\ttest-mlogloss:0.915444\n",
      "[90]\ttrain-mlogloss:0.890522\ttest-mlogloss:0.902107\n",
      "[100]\ttrain-mlogloss:0.877129\ttest-mlogloss:0.890021\n",
      "[110]\ttrain-mlogloss:0.864697\ttest-mlogloss:0.878923\n",
      "[120]\ttrain-mlogloss:0.853693\ttest-mlogloss:0.869292\n",
      "[130]\ttrain-mlogloss:0.843265\ttest-mlogloss:0.860148\n",
      "[140]\ttrain-mlogloss:0.833738\ttest-mlogloss:0.851922\n",
      "[150]\ttrain-mlogloss:0.824766\ttest-mlogloss:0.84441\n",
      "[160]\ttrain-mlogloss:0.81634\ttest-mlogloss:0.837418\n",
      "[170]\ttrain-mlogloss:0.808529\ttest-mlogloss:0.830934\n",
      "[180]\ttrain-mlogloss:0.801277\ttest-mlogloss:0.825186\n",
      "[190]\ttrain-mlogloss:0.794458\ttest-mlogloss:0.819602\n",
      "[200]\ttrain-mlogloss:0.788034\ttest-mlogloss:0.814518\n",
      "[210]\ttrain-mlogloss:0.782132\ttest-mlogloss:0.809975\n",
      "[220]\ttrain-mlogloss:0.776247\ttest-mlogloss:0.805554\n",
      "[230]\ttrain-mlogloss:0.770739\ttest-mlogloss:0.80146\n",
      "[240]\ttrain-mlogloss:0.765716\ttest-mlogloss:0.797757\n",
      "[250]\ttrain-mlogloss:0.760573\ttest-mlogloss:0.794032\n",
      "[260]\ttrain-mlogloss:0.756156\ttest-mlogloss:0.790858\n",
      "[270]\ttrain-mlogloss:0.751852\ttest-mlogloss:0.78789\n",
      "[280]\ttrain-mlogloss:0.747514\ttest-mlogloss:0.78494\n",
      "[290]\ttrain-mlogloss:0.743414\ttest-mlogloss:0.782138\n",
      "[300]\ttrain-mlogloss:0.739516\ttest-mlogloss:0.779538\n",
      "[310]\ttrain-mlogloss:0.735731\ttest-mlogloss:0.777116\n",
      "[320]\ttrain-mlogloss:0.732253\ttest-mlogloss:0.77498\n",
      "[330]\ttrain-mlogloss:0.729012\ttest-mlogloss:0.772915\n",
      "[340]\ttrain-mlogloss:0.725402\ttest-mlogloss:0.770733\n",
      "[350]\ttrain-mlogloss:0.722203\ttest-mlogloss:0.768614\n",
      "[360]\ttrain-mlogloss:0.719186\ttest-mlogloss:0.766872\n",
      "[370]\ttrain-mlogloss:0.71622\ttest-mlogloss:0.765145\n",
      "[380]\ttrain-mlogloss:0.713253\ttest-mlogloss:0.763422\n",
      "[390]\ttrain-mlogloss:0.710588\ttest-mlogloss:0.761975\n",
      "[400]\ttrain-mlogloss:0.707903\ttest-mlogloss:0.760469\n",
      "[410]\ttrain-mlogloss:0.705284\ttest-mlogloss:0.758997\n",
      "[420]\ttrain-mlogloss:0.702842\ttest-mlogloss:0.757748\n",
      "[430]\ttrain-mlogloss:0.700433\ttest-mlogloss:0.756497\n",
      "[440]\ttrain-mlogloss:0.698207\ttest-mlogloss:0.755415\n",
      "[450]\ttrain-mlogloss:0.69614\ttest-mlogloss:0.754434\n",
      "[460]\ttrain-mlogloss:0.693857\ttest-mlogloss:0.753294\n",
      "[470]\ttrain-mlogloss:0.69172\ttest-mlogloss:0.752297\n",
      "[480]\ttrain-mlogloss:0.689666\ttest-mlogloss:0.751305\n",
      "[490]\ttrain-mlogloss:0.687727\ttest-mlogloss:0.750497\n",
      "[500]\ttrain-mlogloss:0.685857\ttest-mlogloss:0.749688\n",
      "[510]\ttrain-mlogloss:0.684158\ttest-mlogloss:0.748856\n",
      "[520]\ttrain-mlogloss:0.682242\ttest-mlogloss:0.748114\n",
      "[530]\ttrain-mlogloss:0.680394\ttest-mlogloss:0.747299\n",
      "[540]\ttrain-mlogloss:0.678604\ttest-mlogloss:0.746549\n",
      "[550]\ttrain-mlogloss:0.676922\ttest-mlogloss:0.745889\n",
      "[560]\ttrain-mlogloss:0.675282\ttest-mlogloss:0.745219\n",
      "[570]\ttrain-mlogloss:0.673662\ttest-mlogloss:0.744566\n",
      "[580]\ttrain-mlogloss:0.671877\ttest-mlogloss:0.74381\n",
      "[590]\ttrain-mlogloss:0.67033\ttest-mlogloss:0.74317\n",
      "[600]\ttrain-mlogloss:0.668876\ttest-mlogloss:0.742636\n",
      "[610]\ttrain-mlogloss:0.66722\ttest-mlogloss:0.741921\n",
      "[620]\ttrain-mlogloss:0.665862\ttest-mlogloss:0.741374\n",
      "[630]\ttrain-mlogloss:0.664539\ttest-mlogloss:0.740866\n",
      "[640]\ttrain-mlogloss:0.663268\ttest-mlogloss:0.740424\n",
      "[650]\ttrain-mlogloss:0.661934\ttest-mlogloss:0.739927\n",
      "[660]\ttrain-mlogloss:0.660367\ttest-mlogloss:0.739313\n",
      "[670]\ttrain-mlogloss:0.659027\ttest-mlogloss:0.738852\n",
      "[680]\ttrain-mlogloss:0.657741\ttest-mlogloss:0.738363\n",
      "[690]\ttrain-mlogloss:0.656426\ttest-mlogloss:0.737912\n",
      "[700]\ttrain-mlogloss:0.655199\ttest-mlogloss:0.737507\n",
      "[710]\ttrain-mlogloss:0.653825\ttest-mlogloss:0.737074\n",
      "[720]\ttrain-mlogloss:0.652502\ttest-mlogloss:0.736645\n",
      "[730]\ttrain-mlogloss:0.651235\ttest-mlogloss:0.736282\n",
      "[740]\ttrain-mlogloss:0.649947\ttest-mlogloss:0.735949\n",
      "[750]\ttrain-mlogloss:0.648615\ttest-mlogloss:0.735469\n",
      "[760]\ttrain-mlogloss:0.647444\ttest-mlogloss:0.73522\n",
      "[770]\ttrain-mlogloss:0.646408\ttest-mlogloss:0.734973\n",
      "[780]\ttrain-mlogloss:0.645276\ttest-mlogloss:0.734589\n",
      "[790]\ttrain-mlogloss:0.644093\ttest-mlogloss:0.734263\n",
      "[800]\ttrain-mlogloss:0.643014\ttest-mlogloss:0.734006\n",
      "[810]\ttrain-mlogloss:0.641873\ttest-mlogloss:0.733635\n",
      "[820]\ttrain-mlogloss:0.640725\ttest-mlogloss:0.733311\n",
      "[830]\ttrain-mlogloss:0.639702\ttest-mlogloss:0.73312\n",
      "[840]\ttrain-mlogloss:0.638636\ttest-mlogloss:0.732821\n",
      "[850]\ttrain-mlogloss:0.637642\ttest-mlogloss:0.732592\n",
      "[860]\ttrain-mlogloss:0.636538\ttest-mlogloss:0.732334\n",
      "[870]\ttrain-mlogloss:0.63564\ttest-mlogloss:0.732117\n",
      "[880]\ttrain-mlogloss:0.634561\ttest-mlogloss:0.731933\n",
      "[890]\ttrain-mlogloss:0.633748\ttest-mlogloss:0.731785\n",
      "[900]\ttrain-mlogloss:0.632827\ttest-mlogloss:0.731567\n",
      "[910]\ttrain-mlogloss:0.631851\ttest-mlogloss:0.731347\n",
      "[920]\ttrain-mlogloss:0.630909\ttest-mlogloss:0.73117\n",
      "[930]\ttrain-mlogloss:0.629951\ttest-mlogloss:0.730996\n",
      "[940]\ttrain-mlogloss:0.629044\ttest-mlogloss:0.730819\n",
      "[950]\ttrain-mlogloss:0.628166\ttest-mlogloss:0.730652\n",
      "[960]\ttrain-mlogloss:0.627112\ttest-mlogloss:0.730384\n",
      "[970]\ttrain-mlogloss:0.626225\ttest-mlogloss:0.73025\n",
      "[980]\ttrain-mlogloss:0.625423\ttest-mlogloss:0.73013\n",
      "[990]\ttrain-mlogloss:0.624451\ttest-mlogloss:0.729932\n",
      "[1000]\ttrain-mlogloss:0.623448\ttest-mlogloss:0.729788\n",
      "[1010]\ttrain-mlogloss:0.622512\ttest-mlogloss:0.729631\n",
      "[1020]\ttrain-mlogloss:0.621525\ttest-mlogloss:0.729431\n",
      "[1030]\ttrain-mlogloss:0.620625\ttest-mlogloss:0.729263\n",
      "[1040]\ttrain-mlogloss:0.619848\ttest-mlogloss:0.729128\n",
      "[1050]\ttrain-mlogloss:0.618886\ttest-mlogloss:0.729008\n",
      "[1060]\ttrain-mlogloss:0.618058\ttest-mlogloss:0.728855\n",
      "[1070]\ttrain-mlogloss:0.617108\ttest-mlogloss:0.728643\n",
      "[1080]\ttrain-mlogloss:0.616304\ttest-mlogloss:0.728479\n",
      "[1090]\ttrain-mlogloss:0.615505\ttest-mlogloss:0.728386\n",
      "[1100]\ttrain-mlogloss:0.614754\ttest-mlogloss:0.728332\n",
      "[1110]\ttrain-mlogloss:0.613952\ttest-mlogloss:0.728229\n",
      "[1120]\ttrain-mlogloss:0.613212\ttest-mlogloss:0.728225\n",
      "[1130]\ttrain-mlogloss:0.61247\ttest-mlogloss:0.728123\n",
      "[1140]\ttrain-mlogloss:0.611644\ttest-mlogloss:0.728026\n",
      "[1150]\ttrain-mlogloss:0.610751\ttest-mlogloss:0.727918\n",
      "[1160]\ttrain-mlogloss:0.610131\ttest-mlogloss:0.727861\n",
      "[1170]\ttrain-mlogloss:0.609427\ttest-mlogloss:0.727718\n",
      "[1180]\ttrain-mlogloss:0.608595\ttest-mlogloss:0.727552\n",
      "[1190]\ttrain-mlogloss:0.607775\ttest-mlogloss:0.727444\n",
      "[1200]\ttrain-mlogloss:0.60702\ttest-mlogloss:0.727349\n",
      "[1210]\ttrain-mlogloss:0.60624\ttest-mlogloss:0.727328\n",
      "[1220]\ttrain-mlogloss:0.605343\ttest-mlogloss:0.727191\n",
      "[1230]\ttrain-mlogloss:0.604351\ttest-mlogloss:0.727031\n",
      "[1240]\ttrain-mlogloss:0.603579\ttest-mlogloss:0.72699\n",
      "[1250]\ttrain-mlogloss:0.602732\ttest-mlogloss:0.72687\n",
      "[1260]\ttrain-mlogloss:0.601918\ttest-mlogloss:0.726771\n",
      "[1270]\ttrain-mlogloss:0.601172\ttest-mlogloss:0.726658\n",
      "[1280]\ttrain-mlogloss:0.600438\ttest-mlogloss:0.726568\n",
      "[1290]\ttrain-mlogloss:0.599674\ttest-mlogloss:0.726496\n",
      "[1300]\ttrain-mlogloss:0.598974\ttest-mlogloss:0.726383\n",
      "[1310]\ttrain-mlogloss:0.598238\ttest-mlogloss:0.726287\n",
      "[1320]\ttrain-mlogloss:0.597435\ttest-mlogloss:0.726206\n",
      "[1330]\ttrain-mlogloss:0.596779\ttest-mlogloss:0.726169\n",
      "[1340]\ttrain-mlogloss:0.595976\ttest-mlogloss:0.726072\n",
      "[1350]\ttrain-mlogloss:0.595251\ttest-mlogloss:0.726025\n",
      "[1360]\ttrain-mlogloss:0.59451\ttest-mlogloss:0.725927\n",
      "[1370]\ttrain-mlogloss:0.593755\ttest-mlogloss:0.725871\n",
      "[1380]\ttrain-mlogloss:0.592933\ttest-mlogloss:0.725782\n",
      "[1390]\ttrain-mlogloss:0.592106\ttest-mlogloss:0.725617\n",
      "[1400]\ttrain-mlogloss:0.591402\ttest-mlogloss:0.725525\n",
      "[1410]\ttrain-mlogloss:0.590687\ttest-mlogloss:0.72539\n",
      "[1420]\ttrain-mlogloss:0.589973\ttest-mlogloss:0.725346\n",
      "[1430]\ttrain-mlogloss:0.58926\ttest-mlogloss:0.725315\n",
      "[1440]\ttrain-mlogloss:0.588621\ttest-mlogloss:0.725297\n",
      "[1450]\ttrain-mlogloss:0.587966\ttest-mlogloss:0.725221\n",
      "[1460]\ttrain-mlogloss:0.587229\ttest-mlogloss:0.725163\n",
      "[1470]\ttrain-mlogloss:0.586622\ttest-mlogloss:0.725142\n",
      "[1480]\ttrain-mlogloss:0.585889\ttest-mlogloss:0.725048\n",
      "[1490]\ttrain-mlogloss:0.585128\ttest-mlogloss:0.724968\n",
      "[1500]\ttrain-mlogloss:0.584443\ttest-mlogloss:0.724908\n",
      "[1510]\ttrain-mlogloss:0.583696\ttest-mlogloss:0.724923\n",
      "[1520]\ttrain-mlogloss:0.583032\ttest-mlogloss:0.724834\n",
      "[1530]\ttrain-mlogloss:0.582404\ttest-mlogloss:0.724804\n",
      "[1540]\ttrain-mlogloss:0.581701\ttest-mlogloss:0.724723\n",
      "[1550]\ttrain-mlogloss:0.5811\ttest-mlogloss:0.724617\n",
      "[1560]\ttrain-mlogloss:0.580459\ttest-mlogloss:0.724572\n",
      "[1570]\ttrain-mlogloss:0.579793\ttest-mlogloss:0.724573\n",
      "[1580]\ttrain-mlogloss:0.579179\ttest-mlogloss:0.724585\n",
      "Stopping. Best iteration:\n",
      "[1558]\ttrain-mlogloss:0.580584\ttest-mlogloss:0.724555\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup parameters for xgboost\n",
    "\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softprob'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.01\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 3\n",
    "param['subsample'] = 0.7\n",
    "param['colsample_bytree'] = 0.7\n",
    "param['eval_metric'] = 'mlogloss'\n",
    "\n",
    "bst = {}\n",
    "pred = {}\n",
    "for f in range(5):\n",
    "    watchlist = [ (xg_train[f],'train'), (xg_test[f], 'test') ]\n",
    "    num_round = 2500\n",
    "    bst[f] = xgb.train(param, xg_train[f], num_round, watchlist , verbose_eval = 10, early_stopping_rounds=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_cv_pred = {}\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "xgb_cv_preds = []\n",
    "df_preds = []\n",
    "for f in range(5):\n",
    "    #preds = et[f].predict_proba(X_test[f])\n",
    "    xgb_cv_pred[f] = bst[f].predict( xg_test[f], ntree_limit=bst[f].best_ntree_limit)\n",
    "    df_preds.append(pd.DataFrame(xgb_cv_pred[f]))\n",
    "    df_preds[-1].columns = ['gp_all0', 'gp_all1', 'gp_mixed']\n",
    "    df_preds[-1]['ppl_group_1'] = train_X.iloc[cv_test_index[f]].ppl_group_1.values\n",
    "\n",
    "df_preds_all = pd.concat(df_preds)\n",
    "\n",
    "#train_preds = np.array(df_preds_all.sort_values('ppl_group_1').drop('ppl_group_1', axis=1))\n",
    "\n",
    "#log_loss(gtrain.otype, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gtesta = group_test.copy()\n",
    "xgb_output = xgb.DMatrix(gtesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for f in range(5):\n",
    "    test_preds.append(bst[f].predict(xgb_output, ntree_limit = bst[f].best_ntree_limit))\n",
    "    \n",
    "preds_comb = np.mean([test_preds[j] for j in range(1,5)], axis = 0)\n",
    "\n",
    "df_test_preds = pd.DataFrame(preds_comb)\n",
    "df_test_preds.columns = ['gp_all0', 'gp_all1', 'gp_mixed']\n",
    "df_test_preds['ppl_group_1'] = gtesta.ppl_group_1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_out = pd.concat([df_test_preds, df_preds_all])\n",
    "\n",
    "preds_out.to_csv('group_pred.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_out = preds_out.drop('ppl_group_1', axis = 1)\n",
    "preds_out.columns = ['gp_all0', 'gp_all1', 'gp_mixed', 'ppl_group_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_out.to_csv('group_pred.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_out = pd.concat([df_test_preds, df_preds_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(preds_out['ppl_group_1'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds_out) == len(df_preds_all) +len(df_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gp_all0</th>\n",
       "      <th>gp_all1</th>\n",
       "      <th>gp_mixed</th>\n",
       "      <th>ppl_group_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.156040</td>\n",
       "      <td>0.842834</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.093303</td>\n",
       "      <td>0.633365</td>\n",
       "      <td>0.273332</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.278425</td>\n",
       "      <td>0.460493</td>\n",
       "      <td>0.261082</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.244403</td>\n",
       "      <td>0.590451</td>\n",
       "      <td>0.165146</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.850534</td>\n",
       "      <td>0.120584</td>\n",
       "      <td>0.028882</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.237691</td>\n",
       "      <td>0.760850</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.981711</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>0.010215</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.262060</td>\n",
       "      <td>0.434307</td>\n",
       "      <td>0.303633</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.158581</td>\n",
       "      <td>0.720605</td>\n",
       "      <td>0.120815</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.465105</td>\n",
       "      <td>0.278054</td>\n",
       "      <td>0.256841</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.816504</td>\n",
       "      <td>0.134849</td>\n",
       "      <td>0.048647</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.403561</td>\n",
       "      <td>0.450870</td>\n",
       "      <td>0.145569</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.336848</td>\n",
       "      <td>0.572514</td>\n",
       "      <td>0.090638</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.408039</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.500605</td>\n",
       "      <td>0.498007</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.801122</td>\n",
       "      <td>0.162726</td>\n",
       "      <td>0.036152</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.220348</td>\n",
       "      <td>0.423724</td>\n",
       "      <td>0.355928</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.416759</td>\n",
       "      <td>0.581528</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.066840</td>\n",
       "      <td>0.695975</td>\n",
       "      <td>0.237185</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.049225</td>\n",
       "      <td>0.746209</td>\n",
       "      <td>0.204566</td>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.846122</td>\n",
       "      <td>0.039397</td>\n",
       "      <td>0.114481</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.289756</td>\n",
       "      <td>0.708611</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.365741</td>\n",
       "      <td>0.632720</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.376771</td>\n",
       "      <td>0.621898</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.278533</td>\n",
       "      <td>0.534480</td>\n",
       "      <td>0.186987</td>\n",
       "      <td>776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.349483</td>\n",
       "      <td>0.443902</td>\n",
       "      <td>0.206615</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.484034</td>\n",
       "      <td>0.514487</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.329562</td>\n",
       "      <td>0.555263</td>\n",
       "      <td>0.115175</td>\n",
       "      <td>874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.534728</td>\n",
       "      <td>0.463842</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.978862</td>\n",
       "      <td>0.020853</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5950</th>\n",
       "      <td>0.525321</td>\n",
       "      <td>0.472941</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>33412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5951</th>\n",
       "      <td>0.446442</td>\n",
       "      <td>0.477458</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>32789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5952</th>\n",
       "      <td>0.463091</td>\n",
       "      <td>0.528652</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>46992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5953</th>\n",
       "      <td>0.250289</td>\n",
       "      <td>0.744629</td>\n",
       "      <td>0.005082</td>\n",
       "      <td>50228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5954</th>\n",
       "      <td>0.041461</td>\n",
       "      <td>0.700584</td>\n",
       "      <td>0.257955</td>\n",
       "      <td>10971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>0.470126</td>\n",
       "      <td>0.528603</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5956</th>\n",
       "      <td>0.631952</td>\n",
       "      <td>0.366927</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>34366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5957</th>\n",
       "      <td>0.706693</td>\n",
       "      <td>0.223003</td>\n",
       "      <td>0.070304</td>\n",
       "      <td>9122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5958</th>\n",
       "      <td>0.531324</td>\n",
       "      <td>0.464759</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>42049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5959</th>\n",
       "      <td>0.136593</td>\n",
       "      <td>0.638461</td>\n",
       "      <td>0.224946</td>\n",
       "      <td>16884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5960</th>\n",
       "      <td>0.112273</td>\n",
       "      <td>0.841748</td>\n",
       "      <td>0.045979</td>\n",
       "      <td>44699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5961</th>\n",
       "      <td>0.382066</td>\n",
       "      <td>0.338115</td>\n",
       "      <td>0.279820</td>\n",
       "      <td>43033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5962</th>\n",
       "      <td>0.368103</td>\n",
       "      <td>0.505568</td>\n",
       "      <td>0.126329</td>\n",
       "      <td>25896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963</th>\n",
       "      <td>0.565619</td>\n",
       "      <td>0.433113</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>45816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5964</th>\n",
       "      <td>0.420237</td>\n",
       "      <td>0.469813</td>\n",
       "      <td>0.109950</td>\n",
       "      <td>36196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5965</th>\n",
       "      <td>0.085278</td>\n",
       "      <td>0.849162</td>\n",
       "      <td>0.065559</td>\n",
       "      <td>21920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5966</th>\n",
       "      <td>0.373180</td>\n",
       "      <td>0.564382</td>\n",
       "      <td>0.062438</td>\n",
       "      <td>37868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5967</th>\n",
       "      <td>0.046353</td>\n",
       "      <td>0.781945</td>\n",
       "      <td>0.171702</td>\n",
       "      <td>4078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5968</th>\n",
       "      <td>0.581719</td>\n",
       "      <td>0.417041</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>45755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>0.493082</td>\n",
       "      <td>0.504915</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>6838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>0.477972</td>\n",
       "      <td>0.460885</td>\n",
       "      <td>0.061143</td>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>0.121213</td>\n",
       "      <td>0.809258</td>\n",
       "      <td>0.069529</td>\n",
       "      <td>6892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5972</th>\n",
       "      <td>0.036520</td>\n",
       "      <td>0.758717</td>\n",
       "      <td>0.204763</td>\n",
       "      <td>20760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>0.062877</td>\n",
       "      <td>0.705420</td>\n",
       "      <td>0.231703</td>\n",
       "      <td>29442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5974</th>\n",
       "      <td>0.345705</td>\n",
       "      <td>0.513558</td>\n",
       "      <td>0.140737</td>\n",
       "      <td>38406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5975</th>\n",
       "      <td>0.415578</td>\n",
       "      <td>0.215778</td>\n",
       "      <td>0.368645</td>\n",
       "      <td>5270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5976</th>\n",
       "      <td>0.283514</td>\n",
       "      <td>0.302033</td>\n",
       "      <td>0.414454</td>\n",
       "      <td>30034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>0.770327</td>\n",
       "      <td>0.102656</td>\n",
       "      <td>0.127017</td>\n",
       "      <td>33403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>0.299485</td>\n",
       "      <td>0.326749</td>\n",
       "      <td>0.373766</td>\n",
       "      <td>16847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>0.242001</td>\n",
       "      <td>0.419550</td>\n",
       "      <td>0.338449</td>\n",
       "      <td>32847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34225 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gp_all0   gp_all1  gp_mixed  ppl_group_1\n",
       "0     0.156040  0.842834  0.001126           15\n",
       "1     0.093303  0.633365  0.273332           18\n",
       "2     0.278425  0.460493  0.261082           53\n",
       "3     0.244403  0.590451  0.165146           85\n",
       "4     0.850534  0.120584  0.028882           99\n",
       "5     0.237691  0.760850  0.001458          126\n",
       "6     0.981711  0.008074  0.010215          134\n",
       "7     0.262060  0.434307  0.303633          182\n",
       "8     0.158581  0.720605  0.120815          235\n",
       "9     0.465105  0.278054  0.256841          293\n",
       "10    0.816504  0.134849  0.048647          297\n",
       "11    0.403561  0.450870  0.145569          305\n",
       "12    0.336848  0.572514  0.090638          322\n",
       "13    0.408039  0.590476  0.001485          363\n",
       "14    0.500605  0.498007  0.001388          403\n",
       "15    0.801122  0.162726  0.036152          449\n",
       "16    0.220348  0.423724  0.355928          454\n",
       "17    0.416759  0.581528  0.001713          461\n",
       "18    0.066840  0.695975  0.237185          497\n",
       "19    0.049225  0.746209  0.204566          527\n",
       "20    0.846122  0.039397  0.114481          551\n",
       "21    0.289756  0.708611  0.001633          657\n",
       "22    0.365741  0.632720  0.001539          737\n",
       "23    0.376771  0.621898  0.001331          749\n",
       "24    0.278533  0.534480  0.186987          776\n",
       "25    0.349483  0.443902  0.206615          784\n",
       "26    0.484034  0.514487  0.001479          840\n",
       "27    0.329562  0.555263  0.115175          874\n",
       "28    0.534728  0.463842  0.001430          889\n",
       "29    0.978862  0.020853  0.000285          938\n",
       "...        ...       ...       ...          ...\n",
       "5950  0.525321  0.472941  0.001738        33412\n",
       "5951  0.446442  0.477458  0.076100        32789\n",
       "5952  0.463091  0.528652  0.008257        46992\n",
       "5953  0.250289  0.744629  0.005082        50228\n",
       "5954  0.041461  0.700584  0.257955        10971\n",
       "5955  0.470126  0.528603  0.001272          903\n",
       "5956  0.631952  0.366927  0.001121        34366\n",
       "5957  0.706693  0.223003  0.070304         9122\n",
       "5958  0.531324  0.464759  0.003917        42049\n",
       "5959  0.136593  0.638461  0.224946        16884\n",
       "5960  0.112273  0.841748  0.045979        44699\n",
       "5961  0.382066  0.338115  0.279820        43033\n",
       "5962  0.368103  0.505568  0.126329        25896\n",
       "5963  0.565619  0.433113  0.001268        45816\n",
       "5964  0.420237  0.469813  0.109950        36196\n",
       "5965  0.085278  0.849162  0.065559        21920\n",
       "5966  0.373180  0.564382  0.062438        37868\n",
       "5967  0.046353  0.781945  0.171702         4078\n",
       "5968  0.581719  0.417041  0.001240        45755\n",
       "5969  0.493082  0.504915  0.002003         6838\n",
       "5970  0.477972  0.460885  0.061143        24000\n",
       "5971  0.121213  0.809258  0.069529         6892\n",
       "5972  0.036520  0.758717  0.204763        20760\n",
       "5973  0.062877  0.705420  0.231703        29442\n",
       "5974  0.345705  0.513558  0.140737        38406\n",
       "5975  0.415578  0.215778  0.368645         5270\n",
       "5976  0.283514  0.302033  0.414454        30034\n",
       "5977  0.770327  0.102656  0.127017        33403\n",
       "5978  0.299485  0.326749  0.373766        16847\n",
       "5979  0.242001  0.419550  0.338449        32847\n",
       "\n",
       "[34225 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
