{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After EDA, what we need to do next are listed below:\n",
    "1. Transfrom all string to int or date for later processing\n",
    "2. Group train data with group_1 and extract those groups who have single outcome as first prediction column\n",
    "3. Make a cross validation set based on people_id(delete all repeted date)\n",
    "4. Make a group-date table and make a prediction on both train and test set via interpolation.(Second prediciton column, still have some None value)\n",
    "5. Fill those none value with average score in train set(third prediction column)\n",
    "6. Group dataset with group_1. Transform those categoty variables in the way of frequency.(#type in the group/# this group). Then we get a group table containing all those feature.\n",
    "7. Mutate group date to create a new column called outcome_type.(0 represents all observations are 0, 1 representd all observation are 1, 2 represends mixed value.)\n",
    "8. Make ensemble-based model to predict both cv and test set. At end of this step, we should have gotten a well-performed 4th prediction on both train and test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from itertools import product\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read all data\n",
    "people = pd.read_csv('people.csv')\n",
    "train = pd.read_csv('act_train.csv')\n",
    "test = pd.read_csv('act_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#String in dataset are in the form of \"str int\"\n",
    "#We only want to keep the int part\n",
    "\n",
    "#Get start with people\n",
    "names = []\n",
    "for name in people.columns:\n",
    "    if 'char' in name:\n",
    "        if type(people[name][0]) == str:\n",
    "            people[name] = people[name].str.replace('type ','')\n",
    "        if type(people[name][0]) == np.bool_:\n",
    "            people[name] = people[name]+0\n",
    "    if 'group' in name:\n",
    "        people[name] = people[name].str.replace('group ', '')\n",
    "    if 'people_id' in name:\n",
    "        people[name] = people[name].str.replace('ppl_', '')\n",
    "        names.append(name)\n",
    "    else:\n",
    "        names.append('ppl_' + name)\n",
    "\n",
    "#transform date to date variable\n",
    "people['date'] = pd.to_datetime(people['date'])\n",
    "\n",
    "#change column names\n",
    "people.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then transform act dataset\n",
    "#fill NA\n",
    "train = train.fillna('type 0')\n",
    "test = test.fillna('type 0')\n",
    "test['outcome'] = None\n",
    "\n",
    "train = train.append(test)\n",
    "\n",
    "#transfrom date\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "\n",
    "names = []\n",
    "for name in train.columns:\n",
    "    if name == 'outcome':\n",
    "        names.append(name)\n",
    "        continue\n",
    "    if 'char' in name or 'category' in name:\n",
    "        train[name] = train[name].str.replace('type ','')\n",
    "    if 'activity_id' in name:\n",
    "        train[name] = train[name].str.replace('act2_','')\n",
    "        train[name] = train[name].str.replace('act1_','')\n",
    "    if 'people_id' in name:\n",
    "        train[name] = train[name].str.replace('ppl_', '')\n",
    "        names.append(name)\n",
    "    else:\n",
    "        names.append('act_' + name)\n",
    "        \n",
    "#change columns name\n",
    "train.columns = names\n",
    "\n",
    "#split\n",
    "test = train[train['outcome'].isnull()]\n",
    "train = train[~train['outcome'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge act and people\n",
    "train = pd.merge(train, people, on = 'people_id')\n",
    "test = pd.merge(test, people, on = 'people_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2ed step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we want to apply what we find in EDA: some groups only hava same outcome.\n",
    "#To avoid occurance, we will choose those group who occued over 100 times.\n",
    "all_zero_group = []\n",
    "all_one_group = []\n",
    "other_group = []\n",
    "count = 0\n",
    "#get different group and store them in list\n",
    "for g in train.groupby('ppl_group_1'):\n",
    "    if len(g[1]) > 100:\n",
    "        outcome_mean = g[1]['outcome'].mean()\n",
    "        if outcome_mean == 1:\n",
    "            all_one_group.append( g[0] )\n",
    "        elif outcome_mean == 0:\n",
    "            all_zero_group.append( g[0] )\n",
    "        else:\n",
    "            other_group.append( g[0] )\n",
    "            count += len(g[1])\n",
    "    else:\n",
    "        other_group.append( g[0] )\n",
    "        count += len(g[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = train.append(test)\n",
    "#get all leak data position\n",
    "#leak_one = data['ppl_group_1'].isin(all_zero_group)\n",
    "#leak_zero = data['ppl_group_1'].isin(all_one_group)\n",
    "#data['outcome_leak1'] = data['outcome'].copy()\n",
    "data['outcome_leak1'] = None\n",
    "data['outcome_leak1'][data['ppl_group_1'].isin(all_zero_group)] = 0.05\n",
    "data['outcome_leak1'][data['ppl_group_1'].isin(all_one_group)] = 0.95\n",
    "\n",
    "#Split data into train and test again\n",
    "train = data[~data['outcome'].isnull()]\n",
    "test = data[data['outcome'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We want to build cross validation dataset\n",
    "#By EDA, there are duplicated data in our dataset\n",
    "#We want there are same number of valid observation in each fold.\n",
    "names = []\n",
    "for name in train.columns:\n",
    "    if name != 'act_activity_id':\n",
    "        names.append(name)\n",
    "#unique_train contrains all valid information\n",
    "unique_train = train[~train.duplicated(subset = names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Based on discussion on the forum, it's a good choice to create cv set by people_id\n",
    "#cv_people contains people_id and its observation number and its mean score (used for stratify)\n",
    "cv_people = []\n",
    "for g in unique_train.groupby('people_id'):\n",
    "    cv_people.append([g[0], len(g[1]), g[1]['outcome'].mean()])\n",
    "#sort outcome mean since we want to get stratified set based on outcome\n",
    "cv_people = sorted(cv_people, key = itemgetter(2), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then create cv set based on id and corresponding number of non-repeated activity\n",
    "#create 10-fold containing outcome stratified people_id\n",
    "cv_people_id = []\n",
    "cv_group_count = np.zeros(10)\n",
    "for i in range(10):\n",
    "    cv_people_id.append( [])\n",
    "for g in cv_people:\n",
    "    need_add_index = np.argsort(cv_group_count)[0]\n",
    "    cv_people_id[need_add_index].append(g[0])\n",
    "    cv_group_count[need_add_index] += g[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create train and cv stratified dataset\n",
    "#cv_train contains unqiue information\n",
    "#cv_eval contains all information\n",
    "cv_train = []\n",
    "cv_eval = []\n",
    "cv_eval_tgt = []\n",
    "\n",
    "#cv_eval_dup = []\n",
    "#??\n",
    "#cv_eval_tgt_dup = []\n",
    "\n",
    "for g in cv_people_id:\n",
    "    cv_train.append(unique_train[~unique_train.people_id.isin(g)])\n",
    "    \n",
    "    #cv_eval.append(unique_train[unique_train.people_id.isin(g)])\n",
    "    #cv_eval_tgt.append(unique_train[unique_train.people_id.isin(g)][['act_activity_id', 'outcome']].copy())\n",
    "    \n",
    "    cv_eval.append(train[train.people_id.isin(g)])\n",
    "    cv_eval_tgt.append(train[train.people_id.isin(g)][['act_activity_id', 'outcome'] ].copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#According to EDA, combination of group and date map to unique outcome\n",
    "#Thus, we want to create a group,date table\n",
    "train['act_date'] = pd.to_datetime(train['act_date'])\n",
    "\n",
    "alldays = pd.date_range(min(train['act_date']),  max(train['act_date']), freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function from Kaggle Forum, Thanks to the provider\n",
    "\"\"\"\n",
    "\n",
    "def interpolateFun0(x):\n",
    "    \"\"\"Original script author's function rewritten in Python.\n",
    "    The author interpolates between two known values by averaging them. We\n",
    "    can think of this as 0th order interpolation. \"\"\"\n",
    "\n",
    "    ## TODO: This function could use some optimization. The R version is much faster...\n",
    "    x = x.reset_index(drop=True)\n",
    "    g = x['outcome'].copy() ## g should be a list or a pandas Series.\n",
    "    \n",
    "    global fv\n",
    "\n",
    "    if (g.shape[0] < 3): ## If we have at most two rows.\n",
    "        x['outcome_filled'] = g ## Will be replaced by a mean.\n",
    "#        x['outcome'] = x['filled']\n",
    "        return x\n",
    "    \n",
    "    if np.sum(g.isnull()) == 0:\n",
    "        x['outcome_filled'] = g\n",
    "        return x\n",
    "    \n",
    "    out = g.values.copy()\n",
    "    value_locs = np.where(~g.isnull())[0]\n",
    "    \n",
    "    if len(value_locs) == 0:\n",
    "        x['outcome_filled'] = np.full_like(out, np.nan)\n",
    "#        x['outcome'] = x['filled']\n",
    "        return x\n",
    "    \n",
    "    if len(value_locs) == 1:\n",
    "        fillval = .89 if (g[value_locs[0]] == 1) else .13\n",
    "        fv.append((g[value_locs[0]], fillval))\n",
    "        g[g.isnull()] = fillval\n",
    "\n",
    "        x['outcome_filled'] = g\n",
    "#        x['outcome'] = x['filled']\n",
    "\n",
    "        return x        \n",
    "    \n",
    "    # Fill in beginning (if needed)\n",
    "    if value_locs[0]:\n",
    "        \n",
    "        fillval = .89 if (g[value_locs[0]] == 1) else .13\n",
    "        fv.append((g[value_locs[0]], fillval))\n",
    "        \n",
    "        out[0:value_locs[0]] = fillval\n",
    "\n",
    "    # Interpolate holes in the middle\n",
    "    for i in range(0, len(value_locs) - 1):\n",
    "        beg = value_locs[i]\n",
    "        end = value_locs[i + 1]\n",
    "        \n",
    "        if g[beg] != g[end]:\n",
    "            out[beg+1:end] = np.interp(range(beg+1, end), [beg, end], [g[beg], g[end]])\n",
    "        else:\n",
    "            out[beg+1:end] = g[beg]\n",
    "\n",
    "    # Fill in end (if needed)\n",
    "    if end < (len(g) - 1):\n",
    "        beg = value_locs[-1]\n",
    "        fillval = .89 if (g[beg] == 1) else .13\n",
    "        fv.append((g[beg], fillval))\n",
    "\n",
    "        out[beg+1:] = fillval\n",
    "\n",
    "    x['outcome_filled'] = out\n",
    "#    x['outcome'] = x['filled']\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply this rule via interpolation on this each cross-validation set and check its power\n",
    "fv = []\n",
    "for i in range(10):\n",
    "    #create group-date table for cv set used to be interpolation\n",
    "    #get all group in a cv set\n",
    "    group_name = cv_eval[i]['ppl_group_1'].unique()\n",
    "    cv_gd_table = pd.DataFrame.from_records(product(group_name, alldays))\n",
    "    cv_gd_table.columns = ['ppl_group_1','act_date']\n",
    "\n",
    "    #get known gt-table in the train set\n",
    "    train_gd_table = cv_train[i].groupby(['ppl_group_1', 'act_date'])['outcome'].agg('mean').to_frame().reset_index()\n",
    "    train_gd_table['act_date'] = pd.to_datetime(train_gd_table['act_date'])\n",
    "    #fill cv table by train table\n",
    "    cv_gd_table = pd.merge(cv_gd_table, train_gd_table,on =['ppl_group_1','act_date'],  how = 'left')\n",
    "    \n",
    "    #insert value to the None date in the same group\n",
    "    cv_gd_table = cv_gd_table.groupby('ppl_group_1').apply(interpolateFun0)\n",
    "    cv_gd_table.columns = ['ppl_group_1', 'act_date', 'outcome_leak2', 'outcome_ip']\n",
    "    \n",
    "    cv_eval[i]['act_date'] = pd.to_datetime(cv_eval[i]['act_date'] )\n",
    "    cv_eval[i] = pd.merge(cv_eval[i], cv_gd_table, on = ['ppl_group_1', 'act_date'], how = 'left')\n",
    "    \n",
    "    #Since sometimes, groups in cv set are not in train set. In this case, we can't fill th value. Instead, we can fill all Na with the\n",
    "    #mean value in training set\n",
    "    cv_eval[i]['outcome_filled'] = cv_eval[i]['outcome_ip'].fillna(cv_train[i]['outcome'].mean())\n",
    "\n",
    "del  cv_gd_table\n",
    "del train_gd_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We should append all we get to trainset as new feature\n",
    "#concat eval set together and merge with original train set\n",
    "temp = pd.concat(cv_eval)[['act_activity_id','outcome_leak2', 'outcome_ip', 'outcome_filled']]\n",
    "train = pd.merge(train, temp, on='act_activity_id', how = 'left')\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_name = test['ppl_group_1'].unique()\n",
    "cv_gd_table = pd.DataFrame.from_records(product(group_name, alldays))\n",
    "cv_gd_table.columns = ['ppl_group_1','act_date']\n",
    "\n",
    "train_gd_table = train.groupby(['ppl_group_1', 'act_date'])['outcome'].agg('mean').to_frame().reset_index()\n",
    "train_gd_table['act_date'] = pd.to_datetime(train_gd_table['act_date'])\n",
    "\n",
    "cv_gd_table = pd.merge(cv_gd_table, train_gd_table,on =['ppl_group_1','act_date'],  how = 'left')\n",
    "\n",
    "\n",
    "cv_gd_table = cv_gd_table.groupby('ppl_group_1').apply(interpolateFun0)\n",
    "cv_gd_table.columns = ['ppl_group_1', 'act_date', 'outcome_leak2', 'outcome_ip']\n",
    "\n",
    "test['act_date'] = pd.to_datetime(test['act_date'] )\n",
    "test = pd.merge(test, cv_gd_table, on = ['ppl_group_1', 'act_date'], how = 'left')\n",
    "\n",
    "test['outcome_filled'] = test['outcome_ip'].fillna(train['outcome'].mean())\n",
    "\n",
    "del  cv_gd_table\n",
    "del train_gd_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To fit our data into classifier, we need to transform our category variables via one-hot or tf-idf.\n",
    "data = train.append(test)\n",
    "\n",
    "#According to EDA, category variables include \n",
    "#act: \"act_activity_category\", 'act_char_1' --'act_char_10'\n",
    "#people: 'ppl_group_1', 'ppl_group_1' -- 'ppl_group_9'\n",
    "#'ppl_group_1' -'ppl_group_9' are the detailed activity, 'ppl_group_10' is more vague. Thus it containes more \n",
    "#much more tyep than any one of others. We need to filter some its value.\n",
    "char_10_count = train.groupby('act_char_10')['ppl_char_10'].agg('count').order(ascending = False)\n",
    "\n",
    "#transform all low-frequency type as -1\n",
    "a = data.act_char_10.copy()\n",
    "a[a.isin(char_10_count[char_10_count <= 100].index)] = -1\n",
    "data.act_char_10 = a\n",
    "del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#backup\n",
    "data.act_date = pd.to_datetime(data.act_date).values.astype('datetime64[D]')\n",
    "data.ppl_date = pd.to_datetime(data.ppl_date).values.astype('datetime64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manage date-related variable, mutate them to create more variables\n",
    "#extract the weekday information\n",
    "data['act_dayofweek'] = data['act_date'].copy()\n",
    "data['act_dayofweek'] = data['act_dayofweek'].dt.dayofweek\n",
    "\n",
    "#get the day difference starting with the minimum day\n",
    "\n",
    "#first for act\n",
    "minimum_day = data['act_date'].min()\n",
    "data['act_date_diff'] = data['act_date'].copy()\n",
    "data['act_date_diff'] = (data['act_date_diff'] - minimum_day) / np.timedelta64(1, 'D') \n",
    "\n",
    "#Then for people\n",
    "minimum_day = data['ppl_date'].min()\n",
    "data['ppl_date_diff'] = data['ppl_date'].copy()\n",
    "data['ppl_date_diff'] = (data['ppl_date_diff'] - minimum_day) / np.timedelta64(1, 'D')\n",
    "#Besides, we assume there may be some connection between ppl_date and corresponding act_ppl\n",
    "data['ap_date_diff'] = (data['act_date'] - data['ppl_date']) /  np.timedelta64(1, 'D')\n",
    "\n",
    "train = data[~data['outcome'].isnull()]\n",
    "test = data[data['outcome'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When apply frequency method, avoid being affected by repeated observation. \n",
    "names = []\n",
    "for name in data.columns:\n",
    "    if name != 'act_activity_id':\n",
    "        names.append(name)\n",
    "#unique_train contrains all valid information\n",
    "unique_data = data[~data.duplicated(subset = names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We transform category variable via group_1 and some group only occured in test set. So we don't know  \n",
    "#the actual return value these groups. We want to extract these group names first.\n",
    "test_groups = test.ppl_group_1[~test.ppl_group_1.isin(train.ppl_group_1.unique())].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On the forum, they mentioned a important point that we should mutate more features within same group\n",
    "#After we add date-related feature, it's time to do it.\n",
    "mutate_features = {}\n",
    "mutate_features['ppl_group_1'] = []\n",
    "mutate_features['min_group_pdate'] = []\n",
    "mutate_features['max_group_pdate'] = []\n",
    "mutate_features['min_group_adate'] = []\n",
    "mutate_features['max_group_adate'] = []\n",
    "mutate_features['group_adate_range'] = []\n",
    "mutate_features['group_ppl_number'] = []\n",
    "mutate_features['group_act_number'] = []\n",
    "mutate_features['group_adate_number'] = []\n",
    "\n",
    "for g in data.groupby('ppl_group_1'):\n",
    "    mutate_features['ppl_group_1'] .append( g[0] )\n",
    "    mutate_features['min_group_adate'].append( g[1].act_date_diff.min())\n",
    "    mutate_features['max_group_adate'].append( g[1].act_date_diff.max())\n",
    "    mutate_features['min_group_pdate'].append( g[1].ppl_date_diff.min())\n",
    "    mutate_features['max_group_pdate'].append( g[1].ppl_date_diff.max())\n",
    "    mutate_features['group_adate_range'].append( g[1].act_date_diff.max() - g[1].act_date_diff.min())\n",
    "    mutate_features['group_ppl_number'].append(len(g[1]['people_id'].unique()))\n",
    "    mutate_features['group_act_number'].append(len(g[1]['act_activity_id'].unique()))\n",
    "    mutate_features['group_adate_number'].append(len(g[1]['act_date'].unique()))\n",
    "    \n",
    "group_feature = pd.DataFrame(mutate_features)\n",
    "#not necessary to merge\n",
    "#data = pd.merge(data, ppl_in_group, on='ppl_group_1', how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we want to transform these category variables to the form of frequency(general onehot aren't suitable to this)\n",
    "#frequency is very sensitive to repeated observation.\n",
    "\n",
    "\n",
    "\n",
    "#get all category variables' name\n",
    "names = []\n",
    "for i in data.columns:\n",
    "    if 'char' in i and i != 'ppl_char_38' :\n",
    "        names.append(i)\n",
    "    elif i == 'act_activity_category':\n",
    "        names.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we calculate type frequency within each group for each category variables\n",
    "#create a function to do that:\n",
    "def cal_freq(data, key):\n",
    "    freq = {}\n",
    "    #Get allunique possible value in this feature\n",
    "    #col_names =  ['ppl_group_1'] + ('freq_' + data['ppl_char_5'].unique()).tolist()\n",
    "    col_names = ['ppl_group_1'] + data[key].unique().tolist()\n",
    "    a = data[key].unique().tolist()\n",
    "    for n in col_names:\n",
    "        freq[n] = []\n",
    "    for g in data.groupby('ppl_group_1'):\n",
    "        freq['ppl_group_1'].append(g[0])\n",
    "        #get count and corresponding feature name\n",
    "        c = a[:]\n",
    "        value_count = g[1][key].value_counts()\n",
    "        for tp in value_count.index:\n",
    "            freq[tp].append(value_count[tp] / float(len(g[1][key])))\n",
    "            c.remove(tp)\n",
    "        for n in c:\n",
    "            freq[n].append(0)\n",
    "    freq = pd.DataFrame(freq)\n",
    "    a = []\n",
    "    for i in freq.columns:\n",
    "        if i != 'ppl_group_1':\n",
    "            if freq[i].mean > 0.001:\n",
    "                i = key + \"_\"+str(i)\n",
    "                a.append(i)\n",
    "            else:\n",
    "                freq = freq.drop(i)\n",
    "        else:\n",
    "            a.append(i)\n",
    "    freq.columns = a\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in names:\n",
    "    freq = cal_freq(unique_data, key)\n",
    "    group_feature = pd.merge(group_feature, freq, on= 'ppl_group_1', how = 'left')\n",
    "    del freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then deal with char_38 which is continues variable\n",
    "group_char_38 = {}\n",
    "group_char_38['char_38_mean'] = []\n",
    "group_char_38['char_38_median'] = []\n",
    "group_char_38['char_38_var'] = []\n",
    "group_char_38['ppl_group_1'] = []\n",
    "\n",
    "#assign value\n",
    "for g in unique_data.groupby('ppl_group_1'):\n",
    "    group_char_38['ppl_group_1'].append(g[0])\n",
    "    group_char_38['char_38_mean'].append(g[1]['ppl_char_38'].mean())\n",
    "    group_char_38['char_38_median'].append(g[1]['ppl_char_38'].median())\n",
    "    if len(g[1]) == 1:\n",
    "        group_char_38['char_38_var'].append(0)\n",
    "    else:\n",
    "        group_char_38['char_38_var'].append(g[1]['ppl_char_38'].var())\n",
    "group_char_38 = pd.DataFrame(group_char_38)\n",
    "group_feature = pd.merge(group_feature,group_char_38 , on ='ppl_group_1', how = 'left')\n",
    "del group_char_38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_train =group_feature[~group_feature.ppl_group_1.isin(test_groups)]\n",
    "group_test =group_feature[group_feature.ppl_group_1.isin(test_groups)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We have mutated all features we need. Then we'll divide this group dataframe into train and test. \n",
    "#We can't assign outcome directly for train set, since some group may have different value\n",
    "#But we can transform to all0, all1 and mixed value for all groups\n",
    "group_outcome = {}\n",
    "group_outcome['ppl_group_1'] = []\n",
    "group_outcome['outcome_type'] = []\n",
    "\n",
    "for g in unique_data.groupby('ppl_group_1'):\n",
    "    group_outcome['ppl_group_1'].append(g[0])\n",
    "    m = g[1].outcome.mean()\n",
    "    if m == 1:\n",
    "        group_outcome['outcome_type'].append(1)\n",
    "    elif m == 0:\n",
    "        group_outcome['outcome_type'].append(0)\n",
    "    else:\n",
    "        group_outcome['outcome_type'].append(2)\n",
    "group_outcome = pd.DataFrame(group_outcome)\n",
    "group_train = pd.merge(group_train, group_outcome, on = 'ppl_group_1', how = 'left')\n",
    "del group_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#By now, we have constructed train and test set. \n",
    "#Then create cv set\n",
    "train_y = group_train['outcome_type']\n",
    "train_X = group_train.drop('outcome_type', axis = 1)\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(train_y, 5, test_size=0.2, random_state=42)\n",
    "\n",
    "cv_train_X, cv_train_y = {},{}\n",
    "cv_test_X, cv_test_y = {},{}\n",
    "cv_test_index = {}\n",
    "\n",
    "i = 0\n",
    "for train_index,test_index in sss:\n",
    "    cv_train_X[i], cv_train_y[i] = train_X.iloc[train_index], train_y[train_index]\n",
    "    cv_test_X[i], cv_test_y[i] = train_X.iloc[test_index], train_y[test_index]\n",
    "    cv_test_index[i] = test_index\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#implement classifier on dataset, 5 classifer for 5 split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "rf = {}\n",
    "train_pred1 = []\n",
    "for i in range(len(cv_train_X)):\n",
    "    rf[i] = RandomForestClassifier(warm_start = True, random_state=42) #train same classifier multiple times\n",
    "    for number in range(10,200, 20):\n",
    "        rf[i].set_params(n_estimators = number)\n",
    "        rf[i].fit(cv_train_X[i], cv_train_y[i])\n",
    "    pred = rf[i].predict_proba(cv_test_X[i])\n",
    "    print log_loss(cv_test_y[i], pred)\n",
    "    pred = pd.DataFrame(pred, columns=['all0_pro', 'all1_pro', 'mixed_pro'])\n",
    "    pred['ppl_group_1'] = train_X.iloc[cv_test_index[i]]['ppl_group_1']\n",
    "    train_pred1.append(pred)\n",
    "\n",
    "train_pred1 = pd.concat(train_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "xg_train = {}\n",
    "xg_test = {}\n",
    "for i in range(5):\n",
    "    xg_train[i] = xgb.DMatrix( cv_train_X[i], label= cv_train_y[i])\n",
    "    xg_test[i] = xgb.DMatrix(cv_test_X[i], label= cv_test_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup parameters for xgboost\n",
    "\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softprob'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.01\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 3\n",
    "param['subsample'] = 0.7\n",
    "param['colsample_bytree'] = 0.7\n",
    "param['eval_metric'] = 'mlogloss'\n",
    "\n",
    "bst = {}\n",
    "pred = {}\n",
    "for f in range(5):\n",
    "    watchlist = [ (xg_train[f],'train'), (xg_test[f], 'test') ]\n",
    "    num_round = 2500\n",
    "    bst[f] = xgb.train(param, xg_train[f], num_round, watchlist , verbose_eval = 10, early_stopping_rounds=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_cv_pred = {}\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "xgb_cv_preds = []\n",
    "df_preds = []\n",
    "for f in range(5):\n",
    "    #preds = et[f].predict_proba(X_test[f])\n",
    "    xgb_cv_pred[f] = bst[f].predict( xg_test[f], ntree_limit=bst[f].best_ntree_limit)\n",
    "    df_preds.append(pd.DataFrame(xgb_cv_pred[f]))\n",
    "    df_preds[-1].columns = ['gp_all0', 'gp_all1', 'gp_mixed']\n",
    "    df_preds[-1]['ppl_group_1'] = train_X.iloc[cv_test_index[f]].ppl_group_1.values\n",
    "\n",
    "df_preds_all = pd.concat(df_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gtesta = group_test.copy()\n",
    "xgb_output = xgb.DMatrix(gtesta)\n",
    "test_preds = []\n",
    "for f in range(5):\n",
    "    test_preds.append(bst[f].predict(xgb_output, ntree_limit = bst[f].best_ntree_limit))\n",
    "    \n",
    "preds_comb = np.mean([test_preds[j] for j in range(1,5)], axis = 0)\n",
    "\n",
    "df_test_preds = pd.DataFrame(preds_comb)\n",
    "df_test_preds.columns = ['gp_all0', 'gp_all1', 'gp_mixed']\n",
    "df_test_preds['ppl_group_1'] = gtesta.ppl_group_1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_out = pd.concat([df_test_preds, df_preds_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_out.to_csv('group_pred.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_first =train \n",
    "test_first = test(\"leak_test.csv\") \n",
    "group_outcome = preds_out \n",
    "del train, test, preds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a whole dataset\n",
    "data = train_first.append(test_first)\n",
    "data = pd.merge(data, group_outcome, on = 'ppl_group_1', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['leak_fillmask'] = data['outcome_ip'].isnull()\n",
    "train = data[~data['outcome'].isnull()]\n",
    "test = data[data['outcome'].isnull()]\n",
    "\n",
    "train_1 = train[train_index['first_filter']].copy()\n",
    "\n",
    "cols = train.columns.copy()\n",
    "cols = cols.drop('act_activity_id')\n",
    "\n",
    "train_1_dedup = train_1[~train_1.duplicated(subset = cols)]\n",
    "#train_1_dup = train_1[train_1.duplicated(subset = cols)]\n",
    "\n",
    "train_2 = train_1_dedup[~train_1_dedup['outcome_ip'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "ppl_count = []\n",
    "\n",
    "for g in train_1_dedup.groupby('people_id'):\n",
    "    ppl_count.append([g[0], len(g[1]), g[1].outcome.mean()])\n",
    "ppl_count = sorted(ppl_count, key = itemgetter(2), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_people_id = []\n",
    "cv_group_count = np.zeros(5)\n",
    "for i in range(5):\n",
    "    cv_people_id.append( [])\n",
    "for g in ppl_count:\n",
    "    need_add_index = np.argsort(cv_group_count)[0]\n",
    "    cv_people_id[need_add_index].append(g[0])\n",
    "    cv_group_count[need_add_index] += g[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_train_1 = []\n",
    "cv_eval_1 = []\n",
    "cv_train_2 = []\n",
    "cv_eval_2 = []\n",
    "\n",
    "cv_eval = []\n",
    "\n",
    "for p in cv_people_id:\n",
    "    cv_train_1.append(train_1_dedup[~train_1_dedup['people_id'].isin(p)])\n",
    "    cv_eval_1.append(train_1_dedup[train_1_dedup['people_id'].isin(p)])\n",
    "    cv_train_2.append(cv_train_1[-1][cv_train_1[-1]['outcome_ip'].isnull()])\n",
    "    cv_eval_2.append(cv_eval_1[-1][cv_eval_1[-1]['outcome_ip'].isnull()])\n",
    "    \n",
    "    cv_eval.append(train_1[train_1['people_id'].isin(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_ppl = []\n",
    "for g in cv_eval_1:\n",
    "    unique_ppl.append(g['people_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_matrix(df):\n",
    "    rows = []\n",
    "    matrices = []\n",
    "    \n",
    "    rows.append(df['ppl_char_38'].values)\n",
    "    rows.append(df['ap_date_diff'].values)\n",
    "    rows.append(df['act_date_diff'].values)\n",
    "    rows.append(df['ppl_date_diff'].values)\n",
    "    \n",
    "    rows.append(df['ppl_group_1'].values)\n",
    "    rows.append(df['act_char_10'].values)\n",
    "    \n",
    "    rows.append(df['gp_all0'].values)\n",
    "    rows.append(df['gp_all1'].values)\n",
    "    rows.append(df['gp_mixed'].values)\n",
    "    \n",
    "    #rows.append(df['outcome_leak1'].values)\n",
    "    #rows.append(df['outcome_ip'].values)\n",
    "    \n",
    "    matrices.append(csr_matrix(np.array(rows).T))\n",
    "    return hstack(matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "train_matrix = {}\n",
    "eval_matrix = {}\n",
    "train_dmat = {}\n",
    "eval_dmat = {}\n",
    "\n",
    "eval_all_matrix = {}\n",
    "eval_all_dmat = {}\n",
    "\n",
    "for i in range(5):\n",
    "    train_matrix[i] = build_matrix(cv_train_2[i])\n",
    "    eval_matrix[i] = build_matrix(cv_eval_2[i])\n",
    "    eval_all_matrix[i] = build_matrix(cv_eval[i])\n",
    "    \n",
    "    train_dmat[i] = xgb.DMatrix(train_matrix[i], label=cv_train_2[i].outcome.values, missing=-1)\n",
    "    eval_dmat[i] = xgb.DMatrix(eval_matrix[i], label=cv_eval_2[i].outcome.values, missing=-1)\n",
    "    eval_all_dmat[i] = xgb.DMatrix(eval_all_matrix[i], label=cv_eval[i].outcome.values, missing=-1)\n",
    "    \n",
    "test_matrix = build_matrix(test)\n",
    "dtest = xgb.DMatrix(test_matrix, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function from Kaggle Forum, Thanks to provider\n",
    "def feval_procleak(yhat, y):\n",
    "    if (len(yhat) != len(cv_val_dups[curfold])):\n",
    "        return \"auc\", sklearn.metrics.roc_auc_score(y.get_label(), yhat)\n",
    "    \n",
    "    yhat_f = yhat.copy()\n",
    "    \n",
    "    locs = np.where(~cv_val_dups[curfold].leak_fillmask)\n",
    "    yhat_f[locs] = cv_val_dups[curfold].outcome_filled.values[locs]\n",
    "    \n",
    "    return \"auc\", sklearn.metrics.roc_auc_score(y.get_label(), yhat_f)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def merge_pred(preds, cv_eval):\n",
    "    cv_eval_pred = []\n",
    "    for i in range(5):\n",
    "        cv_eval_pred.append(cv_eval[i][['outcome','outcome_leak1', 'outcome_leak2', 'outcome_ip', 'outcome_filled', 'leak_fillmask']].copy())\n",
    "        \n",
    "        cv_eval_pred[i]['outcome_pred'] = preds[i]\n",
    "        cv_eval_pred[i]['outcome_merge'] = preds[i]\n",
    "        \n",
    "        cv_eval_pred[i]['outcome_merge'] = cv_eval[i]['outcome_ip'][~cv_eval[i]['leak_fillmask']]\n",
    "        \n",
    "        print(i,\n",
    "              roc_auc_score(cv_eval_pred[i]['outcome'].values, cv_eval_pred[i]['outcome_pred'].values),\n",
    "              roc_auc_score(cv_eval_pred[i]['outcome'].values, cv_eval_pred[i]['outcome_merge'].values))\n",
    "        \n",
    "        return pd.concat(cv_eval_pred)\n",
    "\n",
    "def pred_xgb(bst):\n",
    "    preds = []\n",
    "    for i in range(5):\n",
    "        preds.append(bst[i].predict(eval_all_dmat[i], ntree_limit=bst[i].best_ntree_limit))\n",
    "    \n",
    "    output = merge_pred(preds, cv_eval)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {'max_depth':10, 'eta':0.01, 'silent':1, 'objective':'binary:logistic' }\n",
    "param['eval_metric'] = 'auc'\n",
    "param['subsample'] = 0.5\n",
    "param['colsample_bytree']= 0.3\n",
    "param['min_child_weight'] = 1\n",
    "param['max_depth'] = 5\n",
    "param['booster'] = \"gbtree\"\n",
    "param['seed'] = 12345\n",
    "\n",
    "bst_d5 = {}\n",
    "\n",
    "for curfold in range(5):\n",
    "    watchlist  = [(train_dmat[curfold],'train'), (eval_dmat[curfold], 'eval')]\n",
    "    num_round = 2000\n",
    "    early_stopping_rounds=500\n",
    "    bst_d5[curfold] = xgb.train(param, train_dmat[curfold], num_round, watchlist,\n",
    "                       #feval = feval_procleak,\n",
    "                       early_stopping_rounds=early_stopping_rounds, \n",
    "                       verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_d5 = pred_xgb(bst_d5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
