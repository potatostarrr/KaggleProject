{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At this point, let's list what we have done:\n",
    "1. Transfrom all string to int or date for later processing\n",
    "2. Group train data with group_1 and extract those groups who have single outcome as first prediction column\n",
    "3. Make a cross validation set based on people_id(delete all repeted date)\n",
    "4. Make a group-date table and make a prediction on both train and test set via interpolation.(Second prediciton column, still have some None value)\n",
    "5. Fill those none value with average score in train set(third prediction column)\n",
    "6. Group dataset with group_1. Transform those categoty variables in the way of frequency.(#type in the group/# this group). Then we get a group table containing all those feature.\n",
    "7. Mutate group date to create a new column called outcome_type.(0 represents all observations are 0, 1 representd all observation are 1, 2 represends mixed value.)\n",
    "8. Make ensemble-based model to predict both cv and test set. At end of this step, we should have gotten a well-performed 4th prediction on both train and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read what we got before\n",
    "train_first = pd.read_csv(\"leak_train.csv\") #train data in step2-5\n",
    "test_first = pd.read_csv(\"leak_test.csv\") #test data in step2-5\n",
    "\n",
    "#group_feature = pd.read_csv('group_freq.csv') #feature in each group in step 6\n",
    "group_outcome = pd.read_csv('group_pred.csv') #outcome in each group in step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outcome_leak1: prediction in step 2  \n",
    "outcome_leak2: prediction in step 4(without interpolation)  \n",
    "outcome_ip: prediction in step 4  \n",
    "outcome_fill: prediction in step 5  \n",
    "gp_all0: prob of being all 0 group  \n",
    "gp_all1: prob of being all 1 group  \n",
    "gp_mixed: prob of being mixed group  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_first = train_first.drop('Unnamed: 0', axis = 1)\n",
    "test_first = test_first.drop('Unnamed: 0', axis = 1)\n",
    "#group_outcome['ppl_group_1'] = pd.to_numeric(group_outcome['ppl_group_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a whole dataset\n",
    "data = train_first.append(test_first)\n",
    "data = pd.merge(data, group_outcome, on = 'ppl_group_1', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['leak_fillmask'] = data['outcome_ip'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data[~data['outcome'].isnull()]\n",
    "test = data[data['outcome'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a dictionary to write useful index information of what we get\n",
    "train_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We quiet sure what we \n",
    "train_index['first_filter'] = train['outcome_leak1'].isnull() \n",
    "train_index['second_filter'] = train['outcome_ip'].isnull()\n",
    "\n",
    "#todo\n",
    "#same cv set in step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_1 = train[train_index['first_filter']].copy()\n",
    "\n",
    "cols = train.columns.copy()\n",
    "cols = cols.drop('act_activity_id')\n",
    "\n",
    "train_1_dedup = train_1[~train_1.duplicated(subset = cols)]\n",
    "#train_1_dup = train_1[train_1.duplicated(subset = cols)]\n",
    "\n",
    "train_2 = train_1_dedup[~train_1_dedup['outcome_ip'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "ppl_count = []\n",
    "\n",
    "for g in train_1_dedup.groupby('people_id'):\n",
    "    ppl_count.append([g[0], len(g[1]), g[1].outcome.mean()])\n",
    "ppl_count = sorted(ppl_count, key = itemgetter(2), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_people_id = []\n",
    "cv_group_count = np.zeros(5)\n",
    "for i in range(5):\n",
    "    cv_people_id.append( [])\n",
    "for g in ppl_count:\n",
    "    need_add_index = np.argsort(cv_group_count)[0]\n",
    "    cv_people_id[need_add_index].append(g[0])\n",
    "    cv_group_count[need_add_index] += g[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_train_1 = []\n",
    "cv_eval_1 = []\n",
    "cv_train_2 = []\n",
    "cv_eval_2 = []\n",
    "\n",
    "cv_eval = []\n",
    "\n",
    "for p in cv_people_id:\n",
    "    cv_train_1.append(train_1_dedup[~train_1_dedup['people_id'].isin(p)])\n",
    "    cv_eval_1.append(train_1_dedup[train_1_dedup['people_id'].isin(p)])\n",
    "    cv_train_2.append(cv_train_1[-1][cv_train_1[-1]['outcome_ip'].isnull()])\n",
    "    cv_eval_2.append(cv_eval_1[-1][cv_eval_1[-1]['outcome_ip'].isnull()])\n",
    "    \n",
    "    cv_eval.append(train_1[train_1['people_id'].isin(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_ppl = []\n",
    "for g in cv_eval_1:\n",
    "    unique_ppl.append(g['people_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_matrix(df):\n",
    "    rows = []\n",
    "    matrices = []\n",
    "    \n",
    "    rows.append(df['ppl_char_38'].values)\n",
    "    rows.append(df['ap_date_diff'].values)\n",
    "    rows.append(df['act_date_diff'].values)\n",
    "    rows.append(df['ppl_date_diff'].values)\n",
    "    \n",
    "    rows.append(df['ppl_group_1'].values)\n",
    "    rows.append(df['act_char_10'].values)\n",
    "    \n",
    "    rows.append(df['gp_all0'].values)\n",
    "    rows.append(df['gp_all1'].values)\n",
    "    rows.append(df['gp_mixed'].values)\n",
    "    \n",
    "    #rows.append(df['outcome_leak1'].values)\n",
    "    #rows.append(df['outcome_ip'].values)\n",
    "    \n",
    "    matrices.append(csr_matrix(np.array(rows).T))\n",
    "    return hstack(matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "train_matrix = {}\n",
    "eval_matrix = {}\n",
    "train_dmat = {}\n",
    "eval_dmat = {}\n",
    "\n",
    "eval_all_matrix = {}\n",
    "eval_all_dmat = {}\n",
    "\n",
    "for i in range(5):\n",
    "    train_matrix[i] = build_matrix(cv_train_2[i])\n",
    "    eval_matrix[i] = build_matrix(cv_eval_2[i])\n",
    "    eval_all_matrix[i] = build_matrix(cv_eval[i])\n",
    "    \n",
    "    train_dmat[i] = xgb.DMatrix(train_matrix[i], label=cv_train_2[i].outcome.values, missing=-1)\n",
    "    eval_dmat[i] = xgb.DMatrix(eval_matrix[i], label=cv_eval_2[i].outcome.values, missing=-1)\n",
    "    eval_all_dmat[i] = xgb.DMatrix(eval_all_matrix[i], label=cv_eval[i].outcome.values, missing=-1)\n",
    "    \n",
    "test_matrix = build_matrix(test)\n",
    "dtest = xgb.DMatrix(test_matrix, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These function are copied from Kaggle Forum"
    "def feval_procleak(yhat, y):\n",
    "    if (len(yhat) != len(cv_val_dups[curfold])):\n",
    "        return \"auc\", sklearn.metrics.roc_auc_score(y.get_label(), yhat)\n",
    "    \n",
    "    yhat_f = yhat.copy()\n",
    "    \n",
    "    locs = np.where(~cv_val_dups[curfold].leak_fillmask)\n",
    "    yhat_f[locs] = cv_val_dups[curfold].outcome_filled.values[locs]\n",
    "    \n",
    "    return \"auc\", sklearn.metrics.roc_auc_score(y.get_label(), yhat_f)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def merge_pred(preds, cv_eval):\n",
    "    cv_eval_pred = []\n",
    "    for i in range(5):\n",
    "        cv_eval_pred.append(cv_eval[i][['outcome','outcome_leak1', 'outcome_leak2', 'outcome_ip', 'outcome_filled', 'leak_fillmask']].copy())\n",
    "        \n",
    "        cv_eval_pred[i]['outcome_pred'] = preds[i]\n",
    "        cv_eval_pred[i]['outcome_merge'] = preds[i]\n",
    "        \n",
    "        cv_eval_pred[i]['outcome_merge'] = cv_eval[i]['outcome_ip'][~cv_eval[i]['leak_fillmask']]\n",
    "        \n",
    "        print(i,\n",
    "              roc_auc_score(cv_eval_pred[i]['outcome'].values, cv_eval_pred[i]['outcome_pred'].values),\n",
    "              roc_auc_score(cv_eval_pred[i]['outcome'].values, cv_eval_pred[i]['outcome_merge'].values))\n",
    "        \n",
    "        return pd.concat(cv_eval_pred)\n",
    "\n",
    "def pred_xgb(bst):\n",
    "    preds = []\n",
    "    for i in range(5):\n",
    "        preds.append(bst[i].predict(eval_all_dmat[i], ntree_limit=bst[i].best_ntree_limit))\n",
    "    \n",
    "    output = merge_pred(preds, cv_eval)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.667432\teval-auc:0.644556\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 500 rounds.\n",
      "[50]\ttrain-auc:0.762992\teval-auc:0.717211\n",
      "[100]\ttrain-auc:0.763894\teval-auc:0.713571\n",
      "[150]\ttrain-auc:0.767104\teval-auc:0.715397\n",
      "[200]\ttrain-auc:0.770285\teval-auc:0.716949\n",
      "[250]\ttrain-auc:0.775815\teval-auc:0.71819\n",
      "[300]\ttrain-auc:0.778548\teval-auc:0.718597\n",
      "[350]\ttrain-auc:0.781848\teval-auc:0.719149\n",
      "[400]\ttrain-auc:0.784838\teval-auc:0.71957\n",
      "[450]\ttrain-auc:0.787733\teval-auc:0.719433\n",
      "[500]\ttrain-auc:0.79028\teval-auc:0.719626\n",
      "Stopping. Best iteration:\n",
      "[27]\ttrain-auc:0.759569\teval-auc:0.720437\n",
      "\n",
      "[0]\ttrain-auc:0.672341\teval-auc:0.670879\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 500 rounds.\n",
      "[50]\ttrain-auc:0.758915\teval-auc:0.714092\n",
      "[100]\ttrain-auc:0.76258\teval-auc:0.715771\n",
      "[150]\ttrain-auc:0.769057\teval-auc:0.717978\n",
      "[200]\ttrain-auc:0.772665\teval-auc:0.716776\n",
      "[250]\ttrain-auc:0.775757\teval-auc:0.718319\n",
      "[300]\ttrain-auc:0.778756\teval-auc:0.717856\n",
      "[350]\ttrain-auc:0.780715\teval-auc:0.719258\n",
      "[400]\ttrain-auc:0.782547\teval-auc:0.719815\n",
      "[450]\ttrain-auc:0.784387\teval-auc:0.720609\n",
      "[500]\ttrain-auc:0.786909\teval-auc:0.720471\n",
      "[550]\ttrain-auc:0.789472\teval-auc:0.720422\n",
      "[600]\ttrain-auc:0.791983\teval-auc:0.720241\n",
      "[650]\ttrain-auc:0.794076\teval-auc:0.720365\n",
      "[700]\ttrain-auc:0.796439\teval-auc:0.72009\n",
      "[750]\ttrain-auc:0.797616\teval-auc:0.720048\n",
      "[800]\ttrain-auc:0.799606\teval-auc:0.719864\n",
      "[850]\ttrain-auc:0.800968\teval-auc:0.719399\n",
      "[900]\ttrain-auc:0.802526\teval-auc:0.719355\n",
      "[950]\ttrain-auc:0.804254\teval-auc:0.718865\n",
      "[1000]\ttrain-auc:0.806335\teval-auc:0.71897\n",
      "Stopping. Best iteration:\n",
      "[536]\ttrain-auc:0.78872\teval-auc:0.720783\n",
      "\n",
      "[0]\ttrain-auc:0.592592\teval-auc:0.568098\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 500 rounds.\n",
      "[50]\ttrain-auc:0.746736\teval-auc:0.72159\n",
      "[100]\ttrain-auc:0.750515\teval-auc:0.721836\n",
      "[150]\ttrain-auc:0.751419\teval-auc:0.720495\n",
      "[200]\ttrain-auc:0.755018\teval-auc:0.719349\n",
      "[250]\ttrain-auc:0.75794\teval-auc:0.721587\n",
      "[300]\ttrain-auc:0.761105\teval-auc:0.722743\n",
      "[350]\ttrain-auc:0.763097\teval-auc:0.72276\n",
      "[400]\ttrain-auc:0.763925\teval-auc:0.72241\n",
      "[450]\ttrain-auc:0.765796\teval-auc:0.722326\n",
      "[500]\ttrain-auc:0.767979\teval-auc:0.722162\n",
      "Stopping. Best iteration:\n",
      "[35]\ttrain-auc:0.743382\teval-auc:0.723921\n",
      "\n",
      "[0]\ttrain-auc:0.597615\teval-auc:0.575993\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 500 rounds.\n",
      "[50]\ttrain-auc:0.745791\teval-auc:0.671945\n",
      "[100]\ttrain-auc:0.751129\teval-auc:0.674737\n",
      "[150]\ttrain-auc:0.755239\teval-auc:0.678558\n",
      "[200]\ttrain-auc:0.755613\teval-auc:0.678708\n",
      "[250]\ttrain-auc:0.757192\teval-auc:0.678392\n",
      "[300]\ttrain-auc:0.759895\teval-auc:0.678777\n",
      "[350]\ttrain-auc:0.761492\teval-auc:0.680518\n",
      "[400]\ttrain-auc:0.764229\teval-auc:0.680692\n",
      "[450]\ttrain-auc:0.766684\teval-auc:0.680721\n",
      "[500]\ttrain-auc:0.769224\teval-auc:0.681071\n",
      "[550]\ttrain-auc:0.771396\teval-auc:0.681576\n",
      "[600]\ttrain-auc:0.773592\teval-auc:0.682827\n",
      "[650]\ttrain-auc:0.775855\teval-auc:0.683533\n",
      "[700]\ttrain-auc:0.778167\teval-auc:0.684029\n",
      "[750]\ttrain-auc:0.780428\teval-auc:0.684452\n",
      "[800]\ttrain-auc:0.78227\teval-auc:0.684776\n",
      "[850]\ttrain-auc:0.783971\teval-auc:0.685343\n",
      "[900]\ttrain-auc:0.785293\teval-auc:0.6862\n",
      "[950]\ttrain-auc:0.786812\teval-auc:0.686827\n",
      "[1000]\ttrain-auc:0.788388\teval-auc:0.687219\n",
      "[1050]\ttrain-auc:0.789548\teval-auc:0.687498\n",
      "[1100]\ttrain-auc:0.79129\teval-auc:0.688757\n",
      "[1150]\ttrain-auc:0.792965\teval-auc:0.68954\n",
      "[1200]\ttrain-auc:0.794681\teval-auc:0.689999\n",
      "[1250]\ttrain-auc:0.796745\teval-auc:0.690496\n",
      "[1300]\ttrain-auc:0.798438\teval-auc:0.690963\n",
      "[1350]\ttrain-auc:0.800362\teval-auc:0.691687\n",
      "[1400]\ttrain-auc:0.8024\teval-auc:0.69279\n",
      "[1450]\ttrain-auc:0.804117\teval-auc:0.693254\n",
      "[1500]\ttrain-auc:0.805854\teval-auc:0.693899\n",
      "[1550]\ttrain-auc:0.807613\teval-auc:0.694939\n",
      "[1600]\ttrain-auc:0.808994\teval-auc:0.695745\n",
      "[1650]\ttrain-auc:0.810663\teval-auc:0.696261\n",
      "[1700]\ttrain-auc:0.811957\teval-auc:0.696533\n",
      "[1750]\ttrain-auc:0.813176\teval-auc:0.697145\n",
      "[1800]\ttrain-auc:0.814657\teval-auc:0.697439\n",
      "[1850]\ttrain-auc:0.815865\teval-auc:0.697986\n",
      "[1900]\ttrain-auc:0.817052\teval-auc:0.698277\n",
      "[1950]\ttrain-auc:0.818541\teval-auc:0.698512\n",
      "[0]\ttrain-auc:0.63598\teval-auc:0.606158\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 500 rounds.\n",
      "[50]\ttrain-auc:0.752611\teval-auc:0.733875\n",
      "[100]\ttrain-auc:0.760741\teval-auc:0.737646\n",
      "[150]\ttrain-auc:0.761976\teval-auc:0.737364\n",
      "[200]\ttrain-auc:0.763401\teval-auc:0.736258\n",
      "[250]\ttrain-auc:0.766999\teval-auc:0.733805\n",
      "[300]\ttrain-auc:0.768705\teval-auc:0.732893\n",
      "[350]\ttrain-auc:0.771634\teval-auc:0.731683\n",
      "[400]\ttrain-auc:0.773801\teval-auc:0.730135\n",
      "[450]\ttrain-auc:0.775149\teval-auc:0.730696\n",
      "[500]\ttrain-auc:0.777741\teval-auc:0.730482\n",
      "[550]\ttrain-auc:0.780547\teval-auc:0.729894\n",
      "Stopping. Best iteration:\n",
      "[91]\ttrain-auc:0.759667\teval-auc:0.739162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth':10, 'eta':0.01, 'silent':1, 'objective':'binary:logistic' }\n",
    "param['eval_metric'] = 'auc'\n",
    "param['subsample'] = 0.5\n",
    "param['colsample_bytree']= 0.3\n",
    "param['min_child_weight'] = 1\n",
    "param['max_depth'] = 5\n",
    "param['booster'] = \"gbtree\"\n",
    "param['seed'] = 12345\n",
    "\n",
    "bst_d5 = {}\n",
    "\n",
    "for curfold in range(5):\n",
    "    watchlist  = [(train_dmat[curfold],'train'), (eval_dmat[curfold], 'eval')]\n",
    "    num_round = 2000\n",
    "    early_stopping_rounds=500\n",
    "    bst_d5[curfold] = xgb.train(param, train_dmat[curfold], num_round, watchlist,\n",
    "                       #feval = feval_procleak,\n",
    "                       early_stopping_rounds=early_stopping_rounds, \n",
    "                       verbose_eval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually 3 layers:\n",
    "1. Original train set\n",
    "2. train set filtered by first leak. (delete repeted data when make cv set)\n",
    "3. train set filtered by second leak.\n",
    "\n",
    "Add a new logical column to show, in the layer 3 which evaluation group's observation group_1 not in train set\n",
    "\n",
    "remove ppl_370270/'group 27940' in each cross-validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create Sparse Matrix\n",
    "Choose appropriate feature to create matrix(important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <xgboost.core.Booster at 0x3a78ef60>,\n",
       " 1: <xgboost.core.Booster at 0x143139e8>,\n",
       " 2: <xgboost.core.Booster at 0x143138d0>,\n",
       " 3: <xgboost.core.Booster at 0x143137f0>,\n",
       " 4: <xgboost.core.Booster at 0x14313a58>}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_d5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.67934863730291695, 0.5769955639290143)\n"
     ]
    }
   ],
   "source": [
    "preds_d5 = pred_xgb(bst_d5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
