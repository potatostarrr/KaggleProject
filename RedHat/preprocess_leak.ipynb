{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from itertools import product\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people = pd.read_csv('people.csv')\n",
    "train = pd.read_csv('act_train.csv')\n",
    "test = pd.read_csv('act_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#String in dataset are in the form of \"str int\"\n",
    "#We only want to keep the int part\n",
    "\n",
    "#Get start with people\n",
    "names = []\n",
    "for name in people.columns:\n",
    "    if 'char' in name:\n",
    "        if type(people[name][0]) == str:\n",
    "            people[name] = people[name].str.replace('type ','')\n",
    "        if type(people[name][0]) == np.bool_:\n",
    "            people[name] = people[name]+0\n",
    "    if 'group' in name:\n",
    "        people[name] = people[name].str.replace('group ', '')\n",
    "    if 'people_id' in name:\n",
    "        people[name] = people[name].str.replace('ppl_', '')\n",
    "        names.append(name)\n",
    "    else:\n",
    "        names.append('ppl_' + name)\n",
    "\n",
    "#transform date to date variable\n",
    "people['date'] = pd.to_datetime(people['date'])\n",
    "\n",
    "#change column names\n",
    "people.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Then transform act dataset\n",
    "#fill NA\n",
    "train = train.fillna('type 0')\n",
    "test = test.fillna('type 0')\n",
    "test['outcome'] = None\n",
    "\n",
    "train = train.append(test)\n",
    "\n",
    "#transfrom date\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "\n",
    "names = []\n",
    "for name in train.columns:\n",
    "    if name == 'outcome':\n",
    "        names.append(name)\n",
    "        continue\n",
    "    if 'char' in name or 'category' in name:\n",
    "        train[name] = train[name].str.replace('type ','')\n",
    "    if 'activity_id' in name:\n",
    "        train[name] = train[name].str.replace('act2_','')\n",
    "        train[name] = train[name].str.replace('act1_','')\n",
    "    if 'people_id' in name:\n",
    "        train[name] = train[name].str.replace('ppl_', '')\n",
    "        names.append(name)\n",
    "    else:\n",
    "        names.append('act_' + name)\n",
    "        \n",
    "#change columns name\n",
    "train.columns = names\n",
    "\n",
    "#split\n",
    "test = train[train['outcome'].isnull()]\n",
    "train = train[~train['outcome'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge act and people\n",
    "train = pd.merge(train, people, on = 'people_id')\n",
    "test = pd.merge(test, people, on = 'people_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('train.csv',index= False)\n",
    "test.to_csv('test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to apply what we find in EDA: some groups only hava same outcome.\n",
    "To avoid occurance, we will choose those group who occued over 100 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************\n",
    "******************************\n",
    "******************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_zero_group = []\n",
    "all_one_group = []\n",
    "other_group = []\n",
    "count = 0\n",
    "#get different group and store them in list\n",
    "for g in train.groupby('ppl_group_1'):\n",
    "    if len(g[1]) > 100:\n",
    "        outcome_mean = g[1]['outcome'].mean()\n",
    "        if outcome_mean == 1:\n",
    "            all_one_group.append( g[0] )\n",
    "        elif outcome_mean == 0:\n",
    "            all_zero_group.append( g[0] )\n",
    "        else:\n",
    "            other_group.append( g[0] )\n",
    "            count += len(g[1])\n",
    "    else:\n",
    "        other_group.append( g[0] )\n",
    "        count += len(g[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = train.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get all leak data position\n",
    "#leak_one = data['ppl_group_1'].isin(all_zero_group)\n",
    "#leak_zero = data['ppl_group_1'].isin(all_one_group)\n",
    "#data['outcome_leak1'] = data['outcome'].copy()\n",
    "data['outcome_leak1'] = None\n",
    "data['outcome_leak1'][data['ppl_group_1'].isin(all_zero_group)] = 0.05\n",
    "data['outcome_leak1'][data['ppl_group_1'].isin(all_one_group)] = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split data into train and test again\n",
    "train = data[~data['outcome'].isnull()]\n",
    "test = data[data['outcome'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We want to build cross validation dataset\n",
    "#By EDA, there are duplicated data in our dataset\n",
    "#We want there are same number of valid observation in each fold.\n",
    "names = []\n",
    "for name in train.columns:\n",
    "    if name != 'act_activity_id':\n",
    "        names.append(name)\n",
    "#unique_train contrains all valid information\n",
    "unique_train = train[~train.duplicated(subset = names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Based on discussion on the forum, it's a good choice to create cv set by people_id\n",
    "#cv_people contains people_id and its observation number and its mean score (used for stratify)\n",
    "cv_people = []\n",
    "for g in unique_train.groupby('people_id'):\n",
    "    cv_people.append([g[0], len(g[1]), g[1]['outcome'].mean()])\n",
    "#sort outcome mean since we want to get stratified set based on outcome\n",
    "cv_people = sorted(cv_people, key = itemgetter(2), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Then create cv set based on id and corresponding number of non-repeated activity\n",
    "#create 10-fold containing outcome stratified people_id\n",
    "cv_people_id = []\n",
    "cv_group_count = np.zeros(10)\n",
    "for i in range(10):\n",
    "    cv_people_id.append( [])\n",
    "for g in cv_people:\n",
    "    need_add_index = np.argsort(cv_group_count)[0]\n",
    "    cv_people_id[need_add_index].append(g[0])\n",
    "    cv_group_count[need_add_index] += g[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create train and cv stratified dataset\n",
    "#cv_train contains unqiue information\n",
    "#cv_eval contains all information\n",
    "cv_train = []\n",
    "cv_eval = []\n",
    "cv_eval_tgt = []\n",
    "\n",
    "#cv_eval_dup = []\n",
    "#??\n",
    "#cv_eval_tgt_dup = []\n",
    "\n",
    "for g in cv_people_id:\n",
    "    cv_train.append(unique_train[~unique_train.people_id.isin(g)])\n",
    "    \n",
    "    #cv_eval.append(unique_train[unique_train.people_id.isin(g)])\n",
    "    #cv_eval_tgt.append(unique_train[unique_train.people_id.isin(g)][['act_activity_id', 'outcome']].copy())\n",
    "    \n",
    "    cv_eval.append(train[train.people_id.isin(g)])\n",
    "    cv_eval_tgt.append(train[train.people_id.isin(g)][['act_activity_id', 'outcome'] ].copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#According to EDA, combination of group and date map to unique outcome\n",
    "#Thus, we want to create a group,date table\n",
    "train['act_date'] = pd.to_datetime(train['act_date'])\n",
    "\n",
    "alldays = pd.date_range(min(train['act_date']),  max(train['act_date']), freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function is provided is the kaggle forum\n",
    "def interpolateFun0(x):\n",
    "    \"\"\"Original script author's function rewritten in Python.\n",
    "    The author interpolates between two known values by averaging them. We\n",
    "    can think of this as 0th order interpolation. \"\"\"\n",
    "\n",
    "    ## TODO: This function could use some optimization. The R version is much faster...\n",
    "    x = x.reset_index(drop=True)\n",
    "    g = x['outcome'].copy() ## g should be a list or a pandas Series.\n",
    "    \n",
    "    global fv\n",
    "\n",
    "    if (g.shape[0] < 3): ## If we have at most two rows.\n",
    "        x['outcome_filled'] = g ## Will be replaced by a mean.\n",
    "#        x['outcome'] = x['filled']\n",
    "        return x\n",
    "    \n",
    "    if np.sum(g.isnull()) == 0:\n",
    "        x['outcome_filled'] = g\n",
    "        return x\n",
    "    \n",
    "    out = g.values.copy()\n",
    "    value_locs = np.where(~g.isnull())[0]\n",
    "    \n",
    "    if len(value_locs) == 0:\n",
    "        x['outcome_filled'] = np.full_like(out, np.nan)\n",
    "#        x['outcome'] = x['filled']\n",
    "        return x\n",
    "    \n",
    "    if len(value_locs) == 1:\n",
    "        fillval = .89 if (g[value_locs[0]] == 1) else .13\n",
    "        fv.append((g[value_locs[0]], fillval))\n",
    "        g[g.isnull()] = fillval\n",
    "\n",
    "        x['outcome_filled'] = g\n",
    "#        x['outcome'] = x['filled']\n",
    "\n",
    "        return x        \n",
    "    \n",
    "    # Fill in beginning (if needed)\n",
    "    if value_locs[0]:\n",
    "        \n",
    "        fillval = .89 if (g[value_locs[0]] == 1) else .13\n",
    "        fv.append((g[value_locs[0]], fillval))\n",
    "        \n",
    "        out[0:value_locs[0]] = fillval\n",
    "\n",
    "    # Interpolate holes in the middle\n",
    "    for i in range(0, len(value_locs) - 1):\n",
    "        beg = value_locs[i]\n",
    "        end = value_locs[i + 1]\n",
    "        \n",
    "        if g[beg] != g[end]:\n",
    "            out[beg+1:end] = np.interp(range(beg+1, end), [beg, end], [g[beg], g[end]])\n",
    "        else:\n",
    "            out[beg+1:end] = g[beg]\n",
    "\n",
    "    # Fill in end (if needed)\n",
    "    if end < (len(g) - 1):\n",
    "        beg = value_locs[-1]\n",
    "        fillval = .89 if (g[beg] == 1) else .13\n",
    "        fv.append((g[beg], fillval))\n",
    "\n",
    "        out[beg+1:] = fillval\n",
    "\n",
    "    x['outcome_filled'] = out\n",
    "#    x['outcome'] = x['filled']\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Apply this rule via interpolation on this each cross-validation set and check its power\n",
    "fv = []\n",
    "for i in range(10):\n",
    "    #create group-date table for cv set used to be interpolation\n",
    "    #get all group in a cv set\n",
    "    group_name = cv_eval[i]['ppl_group_1'].unique()\n",
    "    cv_gd_table = pd.DataFrame.from_records(product(group_name, alldays))\n",
    "    cv_gd_table.columns = ['ppl_group_1','act_date']\n",
    "\n",
    "    #get known gt-table in the train set\n",
    "    train_gd_table = cv_train[i].groupby(['ppl_group_1', 'act_date'])['outcome'].agg('mean').to_frame().reset_index()\n",
    "    train_gd_table['act_date'] = pd.to_datetime(train_gd_table['act_date'])\n",
    "    #fill cv table by train table\n",
    "    cv_gd_table = pd.merge(cv_gd_table, train_gd_table,on =['ppl_group_1','act_date'],  how = 'left')\n",
    "    \n",
    "    #insert value to the None date in the same group\n",
    "    cv_gd_table = cv_gd_table.groupby('ppl_group_1').apply(interpolateFun0)\n",
    "    cv_gd_table.columns = ['ppl_group_1', 'act_date', 'outcome_leak2', 'outcome_ip']\n",
    "    \n",
    "    cv_eval[i]['act_date'] = pd.to_datetime(cv_eval[i]['act_date'] )\n",
    "    cv_eval[i] = pd.merge(cv_eval[i], cv_gd_table, on = ['ppl_group_1', 'act_date'], how = 'left')\n",
    "    \n",
    "    #Since sometimes, groups in cv set are not in train set. In this case, we can't fill th value. Instead, we can fill all Na with the\n",
    "    #mean value in training set\n",
    "    cv_eval[i]['outcome_filled'] = cv_eval[i]['outcome_ip'].fillna(cv_train[i]['outcome'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del  cv_gd_table\n",
    "del train_gd_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We should append all we get to trainset as new feature\n",
    "#concat eval set together and merge with original train set\n",
    "temp = pd.concat(cv_eval)[['act_activity_id','outcome_leak2', 'outcome_ip', 'outcome_filled']]\n",
    "train = pd.merge(train, temp, on='act_activity_id', how = 'left')\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "group_name = test['ppl_group_1'].unique()\n",
    "cv_gd_table = pd.DataFrame.from_records(product(group_name, alldays))\n",
    "cv_gd_table.columns = ['ppl_group_1','act_date']\n",
    "\n",
    "train_gd_table = train.groupby(['ppl_group_1', 'act_date'])['outcome'].agg('mean').to_frame().reset_index()\n",
    "train_gd_table['act_date'] = pd.to_datetime(train_gd_table['act_date'])\n",
    "\n",
    "cv_gd_table = pd.merge(cv_gd_table, train_gd_table,on =['ppl_group_1','act_date'],  how = 'left')\n",
    "\n",
    "\n",
    "cv_gd_table = cv_gd_table.groupby('ppl_group_1').apply(interpolateFun0)\n",
    "cv_gd_table.columns = ['ppl_group_1', 'act_date', 'outcome_leak2', 'outcome_ip']\n",
    "\n",
    "test['act_date'] = pd.to_datetime(test['act_date'] )\n",
    "test = pd.merge(test, cv_gd_table, on = ['ppl_group_1', 'act_date'], how = 'left')\n",
    "\n",
    "test['outcome_filled'] = test['outcome_ip'].fillna(train['outcome'].mean())\n",
    "\n",
    "del  cv_gd_table\n",
    "del train_gd_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"leak_train.csv\")\n",
    "test.to_csv(\"leak_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************\n",
    "******************************\n",
    "******************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"leak_train.csv\")\n",
    "test = pd.read_csv(\"leak_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To fit our data into classifier, we need to transform our category variables via one-hot or tf-idf.\n",
    "data = train.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: FutureWarning: order is deprecated, use sort_values(...)\n"
     ]
    }
   ],
   "source": [
    "#According to EDA, category variables include \n",
    "#act: \"act_activity_category\", 'act_char_1' --'act_char_10'\n",
    "#people: 'ppl_group_1', 'ppl_group_1' -- 'ppl_group_9'\n",
    "#'ppl_group_1' -'ppl_group_9' are the detailed activity, 'ppl_group_10' is more vague. Thus it containes more \n",
    "#much more tyep than any one of others. We need to filter some its value.\n",
    "char_10_count = train.groupby('act_char_10')['ppl_char_10'].agg('count').order(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char_10 has 6516 types \n",
      "Maximum count is 935241, minimum count is 1 \n",
      "1076 observations occured over 100 times \n",
      "They contains 0.95612089102% of whole data \n"
     ]
    }
   ],
   "source": [
    "print ('Char_10 has {} types '.format(len(char_10_count)))\n",
    "print ('Maximum count is {0}, minimum count is {1} '.format(max(char_10_count), min(char_10_count) ))\n",
    "print ('{} observations occured over 100 times '.format(  len(char_10_count[char_10_count>100]) ))\n",
    "print ('They contains {}% of whole data '.format(  sum(char_10_count[char_10_count>100]) / float(sum(char_10_count)) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform all low-frequency type as -1\n",
    "a = data.act_char_10.copy()\n",
    "a[a.isin(char_10_count[char_10_count <= 100].index)] = -1\n",
    "data.act_char_10 = a\n",
    "del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#backup\n",
    "data.act_date = pd.to_datetime(data.act_date).values.astype('datetime64[D]')\n",
    "data.ppl_date = pd.to_datetime(data.ppl_date).values.astype('datetime64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#manage date-related variable, mutate them to create more variables\n",
    "#extract the weekday information\n",
    "data['act_dayofweek'] = data['act_date'].copy()\n",
    "data['act_dayofweek'] = data['act_dayofweek'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the day difference starting with the minimum day\n",
    "\n",
    "#first for act\n",
    "minimum_day = data['act_date'].min()\n",
    "data['act_date_diff'] = data['act_date'].copy()\n",
    "data['act_date_diff'] = (data['act_date_diff'] - minimum_day) / np.timedelta64(1, 'D') \n",
    "\n",
    "#Then for people\n",
    "minimum_day = data['ppl_date'].min()\n",
    "data['ppl_date_diff'] = data['ppl_date'].copy()\n",
    "data['ppl_date_diff'] = (data['ppl_date_diff'] - minimum_day) / np.timedelta64(1, 'D')\n",
    "#Besides, we assume there may be some connection between ppl_date and corresponding act_ppl\n",
    "data['ap_date_diff'] = (data['act_date'] - data['ppl_date']) /  np.timedelta64(1, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######\n",
    "#need to be done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>act_activity_id</th>\n",
       "      <th>act_date</th>\n",
       "      <th>act_activity_category</th>\n",
       "      <th>act_char_1</th>\n",
       "      <th>act_char_2</th>\n",
       "      <th>act_char_3</th>\n",
       "      <th>act_char_4</th>\n",
       "      <th>act_char_5</th>\n",
       "      <th>act_char_6</th>\n",
       "      <th>...</th>\n",
       "      <th>ppl_char_37</th>\n",
       "      <th>ppl_char_38</th>\n",
       "      <th>outcome_leak1</th>\n",
       "      <th>outcome_leak2</th>\n",
       "      <th>outcome_ip</th>\n",
       "      <th>outcome_filled</th>\n",
       "      <th>act_dayofweek</th>\n",
       "      <th>act_date_diff</th>\n",
       "      <th>ppl_date_diff</th>\n",
       "      <th>ap_date_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>1734928.0</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>405.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>788.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2434093.0</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>72.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>3404049.0</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>72.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>3651215.0</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>383.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>766.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>4109017.0</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>405.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>788.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   people_id  act_activity_id   act_date  act_activity_category  act_char_1  \\\n",
       "0      100.0        1734928.0 2023-08-26                      4           0   \n",
       "1      100.0        2434093.0 2022-09-27                      2           0   \n",
       "2      100.0        3404049.0 2022-09-27                      2           0   \n",
       "3      100.0        3651215.0 2023-08-04                      2           0   \n",
       "4      100.0        4109017.0 2023-08-26                      2           0   \n",
       "\n",
       "   act_char_2  act_char_3  act_char_4  act_char_5  act_char_6      ...       \\\n",
       "0           0           0           0           0           0      ...        \n",
       "1           0           0           0           0           0      ...        \n",
       "2           0           0           0           0           0      ...        \n",
       "3           0           0           0           0           0      ...        \n",
       "4           0           0           0           0           0      ...        \n",
       "\n",
       "   ppl_char_37  ppl_char_38  outcome_leak1  outcome_leak2  outcome_ip  \\\n",
       "0            0           36           0.05            0.0         0.0   \n",
       "1            0           36           0.05            0.0         0.0   \n",
       "2            0           36           0.05            0.0         0.0   \n",
       "3            0           36           0.05            0.0         0.0   \n",
       "4            0           36           0.05            0.0         0.0   \n",
       "\n",
       "   outcome_filled  act_dayofweek  act_date_diff ppl_date_diff  ap_date_diff  \n",
       "0             0.0              5          405.0         407.0         788.0  \n",
       "1             0.0              1           72.0         407.0         455.0  \n",
       "2             0.0              1           72.0         407.0         455.0  \n",
       "3             0.0              4          383.0         407.0         766.0  \n",
       "4             0.0              5          405.0         407.0         788.0  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#End of this step\n",
    "train = data[~data['outcome'].isnull()]\n",
    "test = data[data['outcome'].isnull()]\n",
    "train.to_csv(\"leak_train.csv\")\n",
    "test.to_csv(\"leak_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
